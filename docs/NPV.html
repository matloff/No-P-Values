<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Norm Matloff">

<title>No P-Values – NPV</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-79108a0fc1995748cbd19a5b0e3e3e7c.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#nhst-is-too-comfortable" id="toc-nhst-is-too-comfortable" class="nav-link active" data-scroll-target="#nhst-is-too-comfortable">NHST Is (Too) Comfortable</a></li>
  <li><a href="#the-central-issue" id="toc-the-central-issue" class="nav-link" data-scroll-target="#the-central-issue">The Central Issue</a>
  <ul class="collapse">
  <li><a href="#effect-of-measurement-error-especially-bias" id="toc-effect-of-measurement-error-especially-bias" class="nav-link" data-scroll-target="#effect-of-measurement-error-especially-bias">Effect of measurement error, especially bias</a></li>
  <li><a href="#both-an-epistemological-and-practical-problem" id="toc-both-an-epistemological-and-practical-problem" class="nav-link" data-scroll-target="#both-an-epistemological-and-practical-problem">Both an epistemological and practical problem</a></li>
  </ul></li>
  <li><a href="#the-easy-part-statistical-vs.-practical-significance-of-treatment-effects" id="toc-the-easy-part-statistical-vs.-practical-significance-of-treatment-effects" class="nav-link" data-scroll-target="#the-easy-part-statistical-vs.-practical-significance-of-treatment-effects">The Easy Part: Statistical vs.&nbsp;Practical Significance of Treatment Effects</a>
  <ul class="collapse">
  <li><a href="#running-example-lsat-data" id="toc-running-example-lsat-data" class="nav-link" data-scroll-target="#running-example-lsat-data">Running example: LSAT data</a></li>
  <li><a href="#wealth-bias-in-the-lsat" id="toc-wealth-bias-in-the-lsat" class="nav-link" data-scroll-target="#wealth-bias-in-the-lsat">Wealth bias in the LSAT?</a></li>
  <li><a href="#source-of-the-problem" id="toc-source-of-the-problem" class="nav-link" data-scroll-target="#source-of-the-problem">Source of the problem</a></li>
  </ul></li>
  <li><a href="#making-a-decision-i" id="toc-making-a-decision-i" class="nav-link" data-scroll-target="#making-a-decision-i">Making a Decision, I</a></li>
  <li><a href="#the-not-so-easy-part-abandon-the-term-significant-altogether" id="toc-the-not-so-easy-part-abandon-the-term-significant-altogether" class="nav-link" data-scroll-target="#the-not-so-easy-part-abandon-the-term-significant-altogether">The Not-So-Easy Part: Abandon the Term <em>Significant</em> Altogether</a>
  <ul class="collapse">
  <li><a href="#example-interaction-effects-in-lsat-data" id="toc-example-interaction-effects-in-lsat-data" class="nav-link" data-scroll-target="#example-interaction-effects-in-lsat-data">Example: interaction effects in LSAT data</a></li>
  </ul></li>
  <li><a href="#making-a-decision-ii" id="toc-making-a-decision-ii" class="nav-link" data-scroll-target="#making-a-decision-ii">Making a Decision, II</a></li>
  <li><a href="#regarding-confidence-intervals" id="toc-regarding-confidence-intervals" class="nav-link" data-scroll-target="#regarding-confidence-intervals">Regarding Confidence Intervals</a></li>
  <li><a href="#formal-non-nhst-tools" id="toc-formal-non-nhst-tools" class="nav-link" data-scroll-target="#formal-non-nhst-tools">Formal Non-NHST Tools</a></li>
  <li><a href="#sec-treats" id="toc-sec-treats" class="nav-link" data-scroll-target="#sec-treats">Procedures Assessing Treatment Effects</a></li>
  <li><a href="#accounting-for-imperfections-in-our-model" id="toc-accounting-for-imperfections-in-our-model" class="nav-link" data-scroll-target="#accounting-for-imperfections-in-our-model">Accounting for Imperfections in Our Model</a>
  <ul class="collapse">
  <li><a href="#dependence-on-ones-goals" id="toc-dependence-on-ones-goals" class="nav-link" data-scroll-target="#dependence-on-ones-goals">Dependence on one’s goals</a>
  <ul class="collapse">
  <li><a href="#sec-something" id="toc-sec-something" class="nav-link" data-scroll-target="#sec-something">We are always estimating <em>something</em></a></li>
  </ul></li>
  <li><a href="#goodness-of-fit-of-the-fundamental-model" id="toc-goodness-of-fit-of-the-fundamental-model" class="nav-link" data-scroll-target="#goodness-of-fit-of-the-fundamental-model">Goodness-of-fit of the fundamental model</a></li>
  <li><a href="#how-can-this-idea-be-implemented" id="toc-how-can-this-idea-be-implemented" class="nav-link" data-scroll-target="#how-can-this-idea-be-implemented">How can this idea be implemented?</a></li>
  <li><a href="#sec-curric" id="toc-sec-curric" class="nav-link" data-scroll-target="#sec-curric">Curricular improvement</a></li>
  </ul></li>
  <li><a href="#sec-whereasa" id="toc-sec-whereasa" class="nav-link" data-scroll-target="#sec-whereasa">Where Is the ASA on This Issue?</a></li>
  <li><a href="#conclusions" id="toc-conclusions" class="nav-link" data-scroll-target="#conclusions">Conclusions</a></li>
  <li><a href="#acknowledgements" id="toc-acknowledgements" class="nav-link" data-scroll-target="#acknowledgements">Acknowledgements</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#simple-applications-of-the-delta-method" id="toc-simple-applications-of-the-delta-method" class="nav-link" data-scroll-target="#simple-applications-of-the-delta-method">Simple Applications of the Delta Method</a>
  <ul class="collapse">
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example">Example</a></li>
  </ul></li>
  <li><a href="#taking-model-inaccuracy-into-account" id="toc-taking-model-inaccuracy-into-account" class="nav-link" data-scroll-target="#taking-model-inaccuracy-into-account">Taking model inaccuracy into account</a>
  <ul class="collapse">
  <li><a href="#example-delta-method" id="toc-example-delta-method" class="nav-link" data-scroll-target="#example-delta-method">Example: Delta Method</a></li>
  <li><a href="#example-modelreality-distance" id="toc-example-modelreality-distance" class="nav-link" data-scroll-target="#example-modelreality-distance">Example: Model/Reality Distance</a></li>
  </ul></li>
  <li><a href="#making-a-decision-iii" id="toc-making-a-decision-iii" class="nav-link" data-scroll-target="#making-a-decision-iii">Making a Decision, III</a>
  <ul class="collapse">
  <li><a href="#ah-this-distance-thing-is-very-useful" id="toc-ah-this-distance-thing-is-very-useful" class="nav-link" data-scroll-target="#ah-this-distance-thing-is-very-useful">Ah, this distance thing is very useful!</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">No P-Values</h1>
<p class="subtitle lead">Why Statistical Hypothesis Testing Is a Bad Idea</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Norm Matloff </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<p><a href="https://heather.cs.ucdavis.edu/matloff.html">Author bio</a></p>
<div class="callout callout-style-default callout-important callout-titled" title="Main Points">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Main Points
</div>
</div>
<div class="callout-body-container callout-body">
<ol type="1">
<li><p>All null hypotheses <span class="math inline">\(H_0\)</span> are false.</p></li>
<li><p>Null Hypothesis Significance Testing (NHST) is thus an absurdity. Why “test” something we know is false?</p></li>
<li><p>Decisions must be made, but based on relevant analysis, not NHST.</p></li>
<li><p>The concept of “practical significance vs.&nbsp;statistical significance” should be reduced to simply assessing practical significance, in all statistical contexts, not just in analyzing treatment effects.</p></li>
<li><p>Usable and interpretable non-NHST tools should be developed, with some being presented here.</p></li>
<li><p>Such tools should not have binary yes/no output, which usurps the analyst’s role and discards valuable information. Instead, should be used to inform the analyst but not automatically make the decision, usurping the analyst.</p></li>
<li><p>This issue has been debated for years, notably among members of the American Statistical Association (<a href="#sec-whereasa" class="quarto-xref">Section&nbsp;11</a>). It’s time to finally repair its serious damage to the field, with the ASA taking a firm lead.</p></li>
</ol>
</div>
</div>
<p><strong>Intended audience:</strong> STEM professionals and researchers (“S” means both natural and social science); statistics educators; mathematical statisticians who wish to modernize the field. A reading knowledge of R is assumed.</p>
<section id="nhst-is-too-comfortable" class="level1">
<h1>NHST Is (Too) Comfortable</h1>
<ul>
<li><p>We all grew up with it.</p></li>
<li><p>Clean, simple, orderly – an OCDer’s dream! :-)</p></li>
<li><p>Highly developed mathematical foundation – a feeling of “safety in math.”</p></li>
</ul>
</section>
<section id="the-central-issue" class="level1">
<h1>The Central Issue</h1>
<p><em>It is foolish to ask “Are the effects of A and B different. They are always different – for some decimal place”</em> – John Tukey</p>
<ul>
<li><p>We wish to determine whether <span class="math inline">\(H_0\)</span> is true or false.</p></li>
<li><p>But:</p></li>
</ul>
<div class="callout callout-style-default callout-important callout-titled" title="Core Problem with NHST">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Core Problem with NHST
</div>
</div>
<div class="callout-body-container callout-body">
<p>In practice, <span class="math inline">\(H_0\)</span> <strong>is never true</strong>.</p>
</div>
</div>
<ul>
<li><p>Examples:</p>
<ul>
<li><p><span class="math inline">\(H_0:\)</span> new drug effectiveness = old drug effectiveness</p>
<p>Can’t be true true after many decimal places.</p></li>
<li><p><span class="math inline">\(H_0:\)</span> <span class="math inline">\(\beta_i = 0\)</span> in linear model</p>
<p>The linear model itself is false – useful maybe but false – but just an approximation.</p></li>
<li><p><span class="math inline">\(H_0:\)</span> population distribution of human weight <span class="math inline">\(W\)</span> is normal/Gaussian</p>
<p>Measured <span class="math inline">\(W\)</span> is discrete, not continuous.</p>
<p>No one has a negative weight, or weighs 10,000 kg.</p></li>
<li><p><span class="math inline">\(H_0:\)</span> Efficient Market Hypothesis</p>
<ul>
<li><p>Example in <a href="https://www.aeaweb.org/articles?id=10.1257/jep.35.3.157">pro-NHST essay</a> by Imbens.</p></li>
<li><p>The EMH is intended only as a rough description, so of course it is false in the exact sense.</p></li>
</ul></li>
</ul></li>
</ul>
<p>Actually, this notion has arisen throughout the long history of the NHST debate.</p>
<section id="effect-of-measurement-error-especially-bias" class="level2">
<h2 class="anchored" data-anchor-id="effect-of-measurement-error-especially-bias">Effect of measurement error, especially bias</h2>
<p>Problems with our data mean that in many cases, <span class="math inline">\(H_0\)</span> is not only false but also simply unanswerable.</p>
<ul>
<li><p>Model imperfections.</p></li>
<li><p>Biased sampling designs.</p></li>
<li><p>Transcription errors.</p></li>
<li><p>“I just want know which medical treatment is better.” <span class="math inline">\(H_0: \mu_1
\leq \mu2\)</span> vs.&nbsp;<span class="math inline">\(H_{\textrm{alt}}: \mu1 &gt; \mu2\)</span>. Sampling bias or data errors can easily push an effect across that threshold, one direction of the other.</p></li>
<li><p>Bias in physical measurements. No physical measurement is infinitely precise, nor absolutely exactly centered.</p></li>
<li><p>E.g. physics, <span class="math inline">\(H_0:\)</span> gravity waves do not exist.</p>
<ul>
<li><p>Yes, the existence is either true or false, but our imperfect measurements make <span class="math inline">\(H_0\)</span> false; with large enough <span class="math inline">\(n\)</span>, we will reject.</p></li>
<li><p>Research on gravity waves relies on a complex array of detection machinery and on other unproven theories. See <a href="https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102#fulltext">the main paper</a> and <a href="https://arxiv.org/pdf/1602.03840">the methods details</a>.</p></li>
<li><p>(The situation is also complicated by the fact that they use a Bayesian analysis, courting further criticism.)</p></li>
</ul></li>
<li><p>Accordingly, replacing classical NHST by <em>mimimal effects tests</em> does not solve the problem.</p></li>
</ul>
</section>
<section id="both-an-epistemological-and-practical-problem" class="level2">
<h2 class="anchored" data-anchor-id="both-an-epistemological-and-practical-problem">Both an epistemological and practical problem</h2>
<ul>
<li><p>OK, <span class="math inline">\(H_0\)</span> is never true. But why is that a problem?</p></li>
<li><p>Philosophical: Unscientific – and just plain silly – to test for a trait that we know <em>a priori</em> is false.</p></li>
<li><p>The American Statistical Association (ASA) rightly takes pride in statistics as a <em>science</em>, <a href="www.amstat.org/about-asa">saying</a> they promote “the development, application, and dissemination of statistical science,” and that its members “[promote] sound statistical practice to inform public policy and improve human welfare.”</p>
<p>But testing a hypothesis that one knows is false certainly isn’t science, nor is it “sound practice.”</p></li>
<li><p>Practical: NHST can be extremely misleading, a grave concern in light of the pivotal role statistics plays in crucial human events, such as in medical cases or legal proceedings.</p></li>
</ul>
</section>
</section>
<section id="the-easy-part-statistical-vs.-practical-significance-of-treatment-effects" class="level1 page-columns page-full">
<h1>The Easy Part: Statistical vs.&nbsp;Practical Significance of Treatment Effects</h1>
<ul>
<li><p>This is the “easy” part, in the sense that most readers here will have learned about this issue in their statistics coursework. In other words, it would seem that they don’t need to be convinced that NHST can be problematic at least in the context of assessing treatment effects.</p></li>
<li><p>Yet they do need to be convinced in general. While they may distinguish between statistical vs.&nbsp;practical significance when analyzing treatment effects, many do not understand the importance of this distinction in general statistical contexts.</p></li>
<li><p>So, let’s look in detail at this “easy” part before delving into deeper issues later in this document.</p></li>
</ul>
<section id="running-example-lsat-data" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="running-example-lsat-data">Running example: LSAT data</h2>
<ul>
<li><p>See <a href="https://www.kaggle.com/datasets/danofer/law-school-admissions-bar-passage">the Kaggle entry</a>.</p></li>
<li><p>Here is an overview of the variables:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">'data/lsa.RData'</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(lsa)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "age"      "decile1"  "decile3"  "fam_inc"  "lsat"     "ugpa"    
 [7] "gender"   "race1"    "cluster"  "fulltime" "bar"     </code></pre>
</div>
</div></li>
<li><p>The two decile scores are class standing in the first and third years of law school, and ‘cluster’ refers to the reputed quality of the law school.</p></li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">The ‘age’ variable is apparently birth year, with e.g.&nbsp;67 meaning 1967.</span></div></div>
<ul>
<li>Two variables of particular interest might be the student’s score on the Law School Admission Test (LSAT) and a logical variable indicating whether the person passed the bar examination.</li>
</ul>
</section>
<section id="wealth-bias-in-the-lsat" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="wealth-bias-in-the-lsat">Wealth bias in the LSAT?</h2>
<ul>
<li>There is concern that the LSAT and other similar tests may be heavily influenced by family income, thus unfair.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>There are important causal issues here and below, but this aspect is beyond the scope of this document.</p>
</div></div><ul>
<li><p>Let’s consider the estimated coefficients in a linear model for the LSAT:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>w <span class="ot">&lt;-</span> <span class="fu">lm</span>(lsat <span class="sc">~</span> .,lsa)  <span class="co"># predict lsat from all other variables</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(w)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = lsat ~ ., data = lsa)

Residuals:
    Min      1Q  Median      3Q     Max 
-19.290  -2.829   0.120   2.888  16.556 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 31.985789   0.448435  71.328  &lt; 2e-16 ***
age          0.020825   0.005842   3.565 0.000365 ***
decile1      0.127548   0.020947   6.089 1.15e-09 ***
decile3      0.214950   0.020919  10.275  &lt; 2e-16 ***
fam_inc      0.300858   0.035953   8.368  &lt; 2e-16 ***
ugpa        -0.278173   0.080431  -3.459 0.000544 ***
gendermale   0.513774   0.060037   8.558  &lt; 2e-16 ***
race1black  -4.748263   0.198088 -23.970  &lt; 2e-16 ***
race1hisp   -2.001460   0.203504  -9.835  &lt; 2e-16 ***
race1other  -0.868031   0.262529  -3.306 0.000947 ***
race1white   1.247088   0.154627   8.065 7.71e-16 ***
cluster2    -5.106684   0.119798 -42.627  &lt; 2e-16 ***
cluster3    -2.436137   0.074744 -32.593  &lt; 2e-16 ***
cluster4     1.210946   0.088478  13.686  &lt; 2e-16 ***
cluster5     3.794275   0.124477  30.482  &lt; 2e-16 ***
cluster6    -5.532161   0.210751 -26.250  &lt; 2e-16 ***
fulltime2   -1.388821   0.116213 -11.951  &lt; 2e-16 ***
barTRUE      1.749733   0.102819  17.018  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.197 on 20782 degrees of freedom
Multiple R-squared:  0.3934,  Adjusted R-squared:  0.3929 
F-statistic: 792.9 on 17 and 20782 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div></li>
<li><p>There are definitely some salient racial aspects here, but, staying with the income issue, look at the coefficient for family income, 0.3009.</p></li>
<li><p>The p-value is essentially 0, which in an academic research journal would be heralded with much fanfare, termed “very highly significant,” with a 3-star insignia.</p></li>
<li><p>But actually, the impact of family income is not significant in practical terms. Here’s why:</p></li>
<li><p>Family income in this dataset is measured in quintiles. So, this estimated coefficient says that, for example, if we compare people who grew up in the bottom 20% of income with those who were raised in the next 20%, the mean LSAT score rises by only about 1/3 of 1 point – on a test where scores are typically in the 20s, 30s and 40s (top score was 48). The 95% confidence interval (CI), (0.2304,0.3714), again indicates that the effect size here is very small.</p></li>
<li><p>So family income is not an important factor after all, and the significance test was highly misleading. The result was statistically significant but not practically significant. How did this discrepancy arise?</p></li>
</ul>
</section>
<section id="source-of-the-problem" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="source-of-the-problem">Source of the problem</h2>
<ul>
<li><p>The above analysis tests the hypothesis</p>
<p><span class="math display">\[
H_0: \beta_{\textrm{faminc}} = 0
\]</span></p>
<p>using the test statistic based on the standard error,</p>
<p><span class="math display">\[
W = \frac{\widehat{\beta}_{\textrm{faminc}}}
{\textrm{s.e.}(\widehat{\beta}_{\textrm{faminc}})}
\]</span></p></li>
<li><p>As the sample size increases the denominator of <span class="math inline">\(W\)</span> goes to 0. The numerator goes to the population parameter <span class="math inline">\(\beta_{\textrm{faminc}}\)</span> – <strong>which is nonzero since all <span class="math inline">\(H_0\)</span>s are false</strong>.</p></li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">In some contexts, the term <em>data-generating process</em>, popular in machine learning circles, may work better than <em>population</em>. I use the latter for simplicity and for its propriety in many contexts.</span></div></div>
<ul>
<li><span class="math inline">\(W\)</span> then goes to <span class="math inline">\(\pm \infty\)</span>, and thus the p-value goes to 0. In other words:</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled" title="The Term 'Significant' Has No Meaning">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Term ‘Significant’ Has No Meaning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Given a large enough dataset, <em>any</em> <span class="math inline">\(H_0\)</span> will have a small p-value and will be rejected. <strong>The term “significant” means nothing.</strong></p>
</div>
</div>
<ul>
<li>The level of “large enough” depends on other factors, e.g.&nbsp;variability in the dataset, complexity of the model and so on, but the same principle holds.</li>
</ul>
</section>
</section>
<section id="making-a-decision-i" class="level1 page-columns page-full">
<h1>Making a Decision, I</h1>
<ul>
<li><p>One of the biggest myths about us critics of NHST is that we fail to recognize that the analyst must make a decision. Of course one must make a decision – but a decision based on relevant information, not on an NHST. “Making a decision” as to whether <span class="math inline">\(H_0\)</span> is true, when we know it to be false, makes no sense.</p></li>
<li><p>For instance:</p>
<blockquote class="blockquote">
<p>In the above analysis, the proper decision is to conclude that family income has negligible effect on LSAT scores, after accounting for other possible factors..</p>
</blockquote></li>
<li><p>As noted at the outset of this document, making decisions in this manner is much less psychologically satisfying than doing an NHST. Rather than the decision being determined automatically in an NHST, the analyst must supply his/her own power, devising an <em>ad hoc</em> criterion for making a decision.</p></li>
<li><p>But it is the scientifically valid approach, rather than relying on “superstition” with NHST.</p></li>
<li><p>It is very telling that in a special 2019 issue of the <em>American Statistician</em> (Volume 73, Issue sup1) devoted to NHST problems, some authors proposed elaborate replacements for NHST. See for instance the proposed SGPV method in by Blume <em>et al</em>. Clearly the psychological urge for an automatic decision-making procedure is clearly very strong. But I would argue that it is counterproductive, not really addressing the problem at hand.</p>
<p></p></li>
</ul>
<div class="no-row-height column-margin column-container"><span class="">See also, for example, Blume <em>et al</em> <em>PLoS One</em>, 2018 13(30</span></div></section>
<section id="the-not-so-easy-part-abandon-the-term-significant-altogether" class="level1">
<h1>The Not-So-Easy Part: Abandon the Term <em>Significant</em> Altogether</h1>
<div class="callout callout-style-default callout-important callout-titled" title="Abandon the Term 'Significant'">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Abandon the Term ‘Significant’
</div>
</div>
<div class="callout-body-container callout-body">
<p>We should focus on “practical significance,” and abandon assessing “[statistical] significance.”</p>
<p>Accordingly we need to identify existing formal tools for assessing <em>practical</em> importance, and develop new such tools.</p>
</div>
</div>
<ul>
<li><p>As noted, today’s statistically-trained analysts know to watch for the phenomenon of statistical significance without practical significance – <strong>in the context of measuring a treatment effect</strong>. In the above case, the “treatment effect” was family income.</p></li>
<li><p>But many analysts do not realize that this same notion may – and should – be extended to non-treatment contexts.</p></li>
<li><p>E.g. an analyst who says, “I just want to use NHST to check whether my model fits the data” is not only missing the principle of “all <span class="math inline">\(H_0\)</span>s are false,” but is also missing an opportunity to perform much more meaningful investigation as to whether the model is useful in the given setting.</p></li>
</ul>
<section id="example-interaction-effects-in-lsat-data" class="level2">
<h2 class="anchored" data-anchor-id="example-interaction-effects-in-lsat-data">Example: interaction effects in LSAT data</h2>
<ul>
<li><p>Say we are considering adding an interaction term between race and undergraduate GPA to our above model. Let’s fit this more elaborate model, then compare.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>w1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(lsat <span class="sc">~</span> .<span class="sc">+</span>race1<span class="sc">:</span>ugpa,lsa)  <span class="co"># add interactions</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(w1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = lsat ~ . + race1:ugpa, data = lsa)

Residuals:
     Min       1Q   Median       3Q      Max 
-19.1783  -2.8065   0.1219   2.8879  16.0633 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     26.574993   1.219611  21.790  &lt; 2e-16 ***
age              0.020612   0.005837   3.531 0.000415 ***
decile1          0.127585   0.020926   6.097 1.10e-09 ***
decile3          0.213918   0.020902  10.234  &lt; 2e-16 ***
fam_inc          0.295042   0.035939   8.210 2.35e-16 ***
ugpa             1.417659   0.363389   3.901 9.60e-05 ***
gendermale       0.513686   0.059986   8.563  &lt; 2e-16 ***
race1black       4.121631   1.439354   2.864 0.004194 ** 
race1hisp        1.378504   1.570833   0.878 0.380191    
race1other       2.212299   1.976702   1.119 0.263073    
race1white       6.838251   1.201559   5.691 1.28e-08 ***
cluster2        -5.105703   0.119879 -42.590  &lt; 2e-16 ***
cluster3        -2.427800   0.074862 -32.430  &lt; 2e-16 ***
cluster4         1.208794   0.088453  13.666  &lt; 2e-16 ***
cluster5         3.777611   0.124422  30.361  &lt; 2e-16 ***
cluster6        -5.565130   0.210945 -26.382  &lt; 2e-16 ***
fulltime2       -1.406151   0.116132 -12.108  &lt; 2e-16 ***
barTRUE          1.743800   0.102855  16.954  &lt; 2e-16 ***
ugpa:race1black -2.876555   0.460281  -6.250 4.20e-10 ***
ugpa:race1hisp  -1.022786   0.494210  -2.070 0.038508 *  
ugpa:race1other -0.941852   0.617940  -1.524 0.127479    
ugpa:race1white -1.737553   0.370283  -4.693 2.72e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 4.193 on 20778 degrees of freedom
Multiple R-squared:  0.3948,  Adjusted R-squared:  0.3942 
F-statistic: 645.4 on 21 and 20778 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div></li>
<li><p>The Black and white interaction terms with undergraduate GPA are “very highly significant.” But does that mean we should include them in our model?</p></li>
<li><p>Let’s check the actual impact of including the interaction terms on the value of <span class="math inline">\(\widehat{E}(Y|X=t)\)</span>, the estimated value of the true regression function <span class="math inline">\(E(Y|X=t)\)</span>, at some point <span class="math inline">\(t\)</span> chosen to be “typical.”</p></li>
<li><p>We will find <span class="math inline">\(\widehat{E}(Y|X=t)\)</span> under both models and compare:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>typx <span class="ot">&lt;-</span> lsa[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">5</span>]  <span class="co"># set up an example case</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(w,typx)  <span class="co"># no-interaction model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      2 
40.2294 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(w1,typx)  <span class="co"># with-interaction model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      2 
40.2056 </code></pre>
</div>
</div></li>
<li><p>Adding the interaction term changed the estimated value of the regression function by only about 0.02 out of a 40.23 baseline.</p>
<p>(The interaction terms themselves are larger than 0.02, but they absorbed some of the values in the original noninteraction variables.)</p></li>
<li><p>So, if our goal is prediction, we should not include the interaction terms.</p></li>
<li><p>On the other hand, in this type of application, our goal is more likely to be assessing treatment effects, the “treatment” here being Black or Hispanic. Then things are more complicated.</p>
<p>We see for instance that a 1.0 increase in GPA reduces the effect of being Black by an estimated -2.876555, resulting in a net effect of 4.121631-2.876555 = 1.245076. On an exam having top score of 48, this is not a strong effect, and we may decide that the effect of being Black is minor in this particular context.</p></li>
</ul>
</section>
</section>
<section id="making-a-decision-ii" class="level1">
<h1>Making a Decision, II</h1>
<ul>
<li><p>Again, the above decision on whether to include interaction terms is not as psychologically satisfying as the NHST approach. We had to supply our own power, devising an <em>ad hoc</em> way to help us make a decision.</p></li>
<li><p>But this is the rational, scientific approach, rather than relying on a test of a hypothesis that we know <em>a priori</em> is false, which is absurdly unscientific.</p></li>
<li><p>In the case of scientific research, the full explanation of the authors’ decision must be made available – especially data (anonymized if need be) and code.</p></li>
</ul>
</section>
<section id="regarding-confidence-intervals" class="level1">
<h1>Regarding Confidence Intervals</h1>
<ul>
<li><p>Never base judgment on whether “the CI contains 0.” That is simply backdoor NHST.</p></li>
<li><p>Instead, use the CI as intended. Pay close attention to the two vital pieces of information contained in the CI:</p>
<ol type="a">
<li><p>Its center tells us our point estimate of the population quantity of interest.</p></li>
<li><p>Its radius gives us an idea of how accurate the point estimate is.</p></li>
</ol></li>
<li><p>Item (b) here is actually familiar to the general, nonstatistical public, in the from of the <em>margin of error</em> in opinion polls.</p></li>
<li><p>If you were a candidate running for office, wouldn’t you want to know (b), not just (a)? You should view statistical analysis in the same way.</p></li>
<li><p>CIs should play a key role in lieu of NHST analyses.</p></li>
<li><p>Of course this includes multiple-comparisons CIs.</p></li>
</ul>
</section>
<section id="formal-non-nhst-tools" class="level1">
<h1>Formal Non-NHST Tools</h1>
<ul>
<li><p>Defenders of NHST emphasize that NHST formally addresses uncertainty. NHST does this badly, but it is definitely true that any analysis must address the random nature of the data. Non-NHST tools must do so.</p></li>
<li><p>So for instance it would be desirable to formalize our assessment of adding interaction terms in the LSAT model above.</p></li>
</ul>
<p>Below we discuss two broad classes of NHST tools. (See the Appendix for mathematical details.)</p>
</section>
<section id="sec-treats" class="level1">
<h1>Procedures Assessing Treatment Effects</h1>
<ul>
<li><p>In the interaction terms example, we informally compared the change in <span class="math inline">\(\widehat{E}(Y|X=t)\)</span> that occurred when we added interaction terms. How can we formalize this into a CI?</p></li>
<li><p>A CI for the contribution of an interaction term to an overall estimate of <span class="math inline">\(E(Y|X=t)\)</span> is straightforward, just the point estimate plus or minux 1.96 times the standard error,</p>
<p><span class="math display">\[
-2.876555 \pm  1.96 \times 0.460281 = (-3.336836, -2.416274)
\]</span></p></li>
<li><p>However, the more relevant quantity might be the net effect of being Black and having a 1.0 point increase in GPA, i.e.&nbsp;the computation 4.121631-2.876555 = 1.245076 shown above. Computation is still straightforward, if the analyst knows covariance matrices (see <a href="#sec-curric" class="quarto-xref">Section&nbsp;10.4</a> below.)</p></li>
<li><p>A more sophisticated approach would be to form a CI for the <em>proportional</em> contribution to <span class="math inline">\(\widehat{E}(Y|X=t)\)</span> of some estimated regression coefficient, for some analyst-chosen “typical” value of <span class="math inline">\(t\)</span>. In the code above, we arbitrarily chose the first row in the dataset as our <span class="math inline">\(t\)</span>, but the analyst might choose it, say, as the vector of means of all the predictor variables.</p>
<p>We then would like to form a CI for the quantity</p>
<p><span class="math display">\[
\frac{\beta_i}{E(Y|X=t)}
\]</span></p>
<p>This can be obtained via the Delta Method, or by the bootstrap.</p></li>
<li><p>The <em>log-linear model</em> can be used to explore relationships among categorical variables. For instance, this might be used in the LSAT data to analyze relationships between race, gender, family income and so on.</p>
<p>Again there is a hierarchy of interaction terms, as in our linear model example, so again either the Delta Method or the bootstrap can be used. The necessary standard errors for DM can be obtained via <a href="https://github.com/matloff/LogLinFit">the “Poisson trick”</a>.</p></li>
</ul>
</section>
<section id="accounting-for-imperfections-in-our-model" class="level1 page-columns page-full">
<h1>Accounting for Imperfections in Our Model</h1>
<p><em>All models are wrong, but some are useful</em> – George Box</p>
<section id="dependence-on-ones-goals" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="dependence-on-ones-goals">Dependence on one’s goals</h2>
<p>As usual, proper analysis depends on one’s goals.</p>
<ul>
<li><p>When an analyst who says, “I just want to use NHST to check whether my model fits the data,” what is the analyst’s goal? For instance, will he/she then apply the model on the strength of <span class="math inline">\(H_0\)</span> being accepted? The test is not designed for that use, and, given enough data, this approach will lead to discarding what may be a very useful model.</p></li>
<li><p>If the goal is prediction, a linear model may be quite effective even though there is a substantial discrepancy between it and the true population regression function <span class="math inline">\(E(Y|X=t)\)</span>.</p></li>
<li><p>But if the goal is measurement of treatment effects, we may be producing distorted results if the model discrepancy is large. Addressing this problem involves two components:</p>
<ul>
<li><p>Clarifying the issue of what is actually being estimated, in light of the inevitable falsity of our model.</p></li>
<li><p>Optionally, formally assessing the degree of discrepancy between our model and reality.</p></li>
<li><p>In the next section, we will elaborate on these last two points.</p></li>
</ul></li>
<li><p>D. Lakens, a highly outspoken defender of p-values, objects to what he believes is statisticians telling others what their goals ought to be. Again, the chosen analysis must be tailored to the analyst’s own goals. This is of course true, but the problem is that p-values, even in modified form, do not address any practical goals.</p>
<p>He mentions for instance that “…the null hypothesis should be plausible enough so that rejecting it is surprising…” Again, the fact that, given a large enough <span class="math inline">\(n\)</span>, a “surprise” will result even in settings in which the analyst would wish not to be surprised.</p></li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">Lakens, <em>Perspect. Psych. Scie, 2021, 16(3)</em></span></div></div>
<div class="callout callout-style-default callout-important callout-titled" title="The Bottom Line">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
The Bottom Line
</div>
</div>
<div class="callout-body-container callout-body">
<p>P-values don’t address the analyst’s goals. The availability of estimating procedures appropriate to those goals is crucial.</p>
</div>
</div>
<section id="sec-something" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="sec-something">We are always estimating <em>something</em></h3>
<ul>
<li><p>Say we are estimating a parameter <span class="math inline">\(\theta\)</span> in some model. Does the inevitable incorrectness of the model mean our estimate is meaningless? No, not at all.</p></li>
<li><p>Even though our model is (always) wrong, we are still estimating <em>something</em>. If we fit a linear regression model <span class="math inline">\(E(Y|X=t) = \beta'X\)</span> (using ’ for matrix transpose, in this case of a column vector), we are in essence estimating the linear function closest to the true population regression function <span class="math inline">\(\mu(t) = E(Y|X=t)\)</span>. What we are estimating with our <span class="math inline">\(\widehat{\beta}\)</span> is then the vector of coefficients for that linear function.</p></li>
</ul>
<div class="page-columns page-full"><p></p><div class="no-row-height column-margin column-container"><span class="">The term “closest” will be used in a broad sense. It could be defined mathematically in various ways.</span></div></div>
<ul>
<li><p>Statistical theory asks what the long-run behavior of an estimator is, as the sample size <span class="math inline">\(n\)</span> grows. When we say that our least-squares estimator <span class="math inline">\(\widehat{\beta}\)</span> estimates that “closest” <span class="math inline">\(\beta\)</span>, we mean that <span class="math inline">\(\lim_{n \rightarrow \infty} \widehat{\beta} =\beta\)</span>.</p></li>
<li><p>The mathematical details are in the Appendix, but the point is this: If that “closest linear function” is not too far from <span class="math inline">\(\mu(t)\)</span>, we can feel comfortable with analyzing the resulting <span class="math inline">\(\widehat{\beta}\)</span> as an approximation.</p></li>
<li><p>The same principle holds for, say, Maximum Likelihood Estimators (MLE). We are estimating the closest model to reality.</p></li>
<li><p>However, our estimation procedure for that closest model should not assume the model is correct, especially in computing standard errors. For instance:</p>
<ul>
<li><p>Normally, in the linear model, we estimate the conditional variance <span class="math inline">\(Var(Y|X=t) = \sigma^2\)</span> using the squared residuals,</p>
<p><span class="math display">\[
\frac{1}{n-p-1} \sum_{i=1}^n (Y_i - \widehat{\beta}' X_i)^2
\]</span></p>
<p>which is then used to produce standard errors for the <span class="math inline">\(\widehat{\beta}_i\)</span>.</p>
<p>But that only works if the model is correct. If not, then each residual includes the model bias. That in turn invalidates the standard errors of the estimated coefficients.</p></li>
<li><p>In the case of MLEs, the standard errors come from the <em>information matrix</em>, based on a derivation that assumes the model is correct. So standard errors based on the classic formula are incorrect.</p></li>
</ul></li>
<li><p>Thus alternative ways of computing standard errors must be found. At the very least, one has the bootstrap, but analytical methods would be preferred. This discussed in the Appendix.</p></li>
</ul>
</section>
</section>
<section id="goodness-of-fit-of-the-fundamental-model" class="level2">
<h2 class="anchored" data-anchor-id="goodness-of-fit-of-the-fundamental-model">Goodness-of-fit of the fundamental model</h2>
<ul>
<li><p>Once again, “all models are wrong,” so in addition to calculating a point estimate and a standard error for <span class="math inline">\(\theta\)</span>, the analyst may optionally wish to assess how close his/her model is to the true population entity.</p></li>
<li><p>Classically, that would be done via NHST, in some sort of goodness-of-fit test, but of course NHST should not be done. One should both calculate a point estimate for the distance and also a standard error for the estimated distance.</p></li>
<li><p>(In many cases, this is done informally through a graphical procedure, e.g.&nbsp;plotting residuals. This is fine, but here we address the issue of formal methods.)</p></li>
</ul>
</section>
<section id="how-can-this-idea-be-implemented" class="level2">
<h2 class="anchored" data-anchor-id="how-can-this-idea-be-implemented">How can this idea be implemented?</h2>
<ul>
<li><p>There is an extensive literature on, and R implementations of, <em>minimum distance estimators</em>, a concept which would seem to dovetail with the above point that with a false model we are estimating the closest instance of our model to the true population entity.</p></li>
<li><p>However, many have limitations from our point of view here. Most important, the procedure must not assume correctness of the model. In addition, we would require that the procedure should produce standard errors not only for <span class="math inline">\(\widehat{\theta}\)</span>, but also for the estimated distance between the asymptotic model fit and the true population process.</p></li>
<li><p>Fortunately what we need can be easily developed for large classes of estimators. See the Appendix.</p></li>
</ul>
</section>
<section id="sec-curric" class="level2">
<h2 class="anchored" data-anchor-id="sec-curric">Curricular improvement</h2>
<p>Besides the obvious retooling of statistics curricula in terms of banishing NHST, statistics programs need to modernize in terms of including some material on multivariate random variables. By this I mean in particular the concept of covariance and covariance matrices.</p>
<p>Students need to be taught how to find the standard error of a linear combination <span class="math inline">\(a' \widehat{\theta}\)</span> of the components of a vector <span class="math inline">\(\widehat{\theta}\)</span>, knowing its estimated covariance matrix <span class="math inline">\(\widehat{\Sigma}\)</span>. They should be familiar with quadratic forms <span class="math inline">\(a'\widehat{\Sigma}a\)</span>, and more generally, <span class="math inline">\(a'\widehat{\Sigma}b\)</span>.</p>
<p>I would surmise that even most Master’s programs in Statistics do not cover this. Yet really the concept is not beyond the grasp of students in methodology courses that do not require linear algebra, say a course in regression for non-Statistics majors. If the course uses the R <strong>lm</strong> function, there is no reason students can’t be taught the associated <strong>vcov</strong> method.</p>
<p>A related topic is the Delta Method. It’s very handy for constructing CIs for nonlinear functions of <span class="math inline">\(\widehat{\theta}\)</span>, but has two main barriers to usage:</p>
<ul>
<li><p>Calculation of covariance matrices.</p></li>
<li><p>Calculation of derivatives.</p></li>
</ul>
<p>That first barrier again illustrates the above point regarding the need for the inclusion of rudimentary properties of covariance matrices in Statistics curricula.</p>
<p>The second barrier is overcome by use of software that computes numerical derivatives internally, such as R’s <strong>msm::deltamethod</strong>.</p>
<p>Some exposure to the bootstrap is also desirable.</p>
<p>Many analysts who apply statistics are largely self-taught. Again, educational materials in that genre should account for these issues.</p>
</section>
</section>
<section id="sec-whereasa" class="level1">
<h1>Where Is the ASA on This Issue?</h1>
<p>The American Statistical Association clearly should play a leading role in rectifying this grave error in statistical science. And in fact, in one sense, it has done so.</p>
<p>In 2016, the ASA released <a href="https://www.burtthompson.net/uploads/9/6/8/4/9684389/wasserstein-2016__asa_p-value_statement.pdf">a position paper</a> on the issue, along with a <a href="https://www.amstat.org/asa/files/pdfs/p-valuestatement.pdf">cover letter.</a>. The latter says in part (emphasis added),</p>
<blockquote class="blockquote">
<p>“‘The p-value was never intended to be a substitute for scientific reasoning,’ said Ron Wasserstein, the ASA’s executive director. ‘Well-reasoned statistical arguments contain much more than the value of a single number and whether that number exceeds an arbitrary threshold. <em>The ASA statement is intended to steer research into a ‘post p&lt;0.05 era</em>’…‘The contents of the ASA statement and the reasoning behind it are not new—statisticians and other scientists have been writing on the topic for decades,’ [then-ASA President] Utts said. ‘But this is the first time that the community of statisticians, as represented by the ASA Board of Directors, has issued a statement to address these issues.’”</p>
</blockquote>
<p>Wasserstein (who today is still the ASA executive director), along with <em>American Statistician</em> editor Nicole Lazar, authored the article.</p>
<p>There is considerable overlap there with my points in the present document. But the ASA editorial (referred to below as “the 2016 ASA Statement”) stopped short of advocating complete abandonment of the p-value concept.</p>
<p>Later, the <em>American Statistician</em> ran a full issue on the topic, with a sharper tone in the <a href="https://www.tandfonline.com/doi/pdf/10.1080/00031305.2019.1583913">cover editorial</a> (which I’ll call “the 2019 editorial”). The authors, Wasserstein, Lazar and Allen Schirm,used language such as “eliminating the use of p-values as a truth arbiter.”</p>
<p>Readers of the present document should keep in mind, as I have noted, that <strong>my criticisms of NHST are much deeper than those of most critics, which don’t go much further than warning analysts of the difference between statistical and practical significance.</strong> In that light, this passsge in the 2019 editorial is especially noteworthy:</p>
<blockquote class="blockquote">
<p>Reviewers of this editorial asked, as some readers of it will, is a p-value threshold ever okay to use? We asked some of the authors of articles in the special issue that question as well…[some] authors suggested that such dichotomized use of p-values was acceptable in modelfitting and variable selection strategies, again as automated tools, this time for sorting through large numbers of potential models or variables. Still others pointed out that p-values with very low thresholds are used in fields such as physics, genomics, and imaging as a filter for massive numbers of tests. ‘[B]inary decision making is indispensable in medicine and related fields’…</p>
</blockquote>
<p><strong>The above passage brings up several issues that I refute in the present document.</strong></p>
<p>According to Wasserstein (personal communication, April 8, 2025), after the release of the 2019 editorial,</p>
<blockquote class="blockquote">
<p>…there were certainly some concerned voices. One of them reached out to Karen Kafadar, who was [ASA] president at the time…[She appointed a task force to study the matter] The task force composition notably lacked diversity of viewpoints on this issue…”</p>
</blockquote>
<p>The 2019 editorial led to <a href="https://hdsr.mitpress.mit.edu/pub/50vl2b07/release/2">a vehement counter-statement</a> by the task force (“the 2021 task force report”), authored by some of the most prominent people in the field. As Wasserstein points out, the list of signatores “notably lacked diversity of viewpoints on this issue.”</p>
<p>I consider that list to include some of the most talented scholars in the field. How could such creative – and thus open-minded – people be so closed-minded on this issue? I believe this stems from the strong emotionally vested feelings on NHST among mathematical statisticians. For them, NHST is not just something tbey “grew up with,” but a notion that has been absolutely core to their careers. Their work and their professional stature have stemmed from research that is typically NHST-centered. They thus had a powerful incentive to counter the 2019 editorial. Though their bias was undoubtely not conscious, it was nevertheless very profound.</p>
<p>I hope that these highly talented researchers will instead turn to developing non- NHST tools.</p>
<p>Finally, it should be noted that neither the 2019 nor 2021 statements are official ASA policy; the 2016 ASA Statement is the only such position. As Wasserstein wrote in his April 2025 message to me,</p>
<blockquote class="blockquote">
<p>“The 2016 ASA Statement on P-Values and Statistical Significance remains the only official statement issued by the ASA on this topic. The president’s task force statement represents the views of its authors, not an official ASA position.”</p>
</blockquote>
</section>
<section id="conclusions" class="level1">
<h1>Conclusions</h1>
<p>In the classic novel <em>Don Quixote</em>, the deranged but well-meaning hero attacks windmills, which he believes to be evil giants. American humorist Mark Twain <a href="en.wikipedia.org/wiki/Don_Quixote">viewed the work</a> as having “swept the world’s admiration for the mediaeval chivalry-silliness out of existence.” The NHST silliness is no less deserving of demise.</p>
<p>Statistics textbooks and curricula should finally be purged of the absurdity of testing hypotheses that we know <em>a priori</em> to be false. Mathematical statisticians should develop new formal non-NHST tools. And most of all, the ASA should play a leading role in all this, expanding on its 2016 ASA Statement and becoming more activist on the issue.</p>
</section>
<section id="acknowledgements" class="level1 page-columns page-full">
<h1>Acknowledgements</h1>
<div class="page-columns page-full"><p>I was first introduced to this vital issue long ago in graduate  school by the late Professor Olive Jean Dunn of the UCLA Biostatistics Department. I was doing my (very theoretical) dissertation in the Mathematics Department. but took classes in Biostat. Jean brought up the topic casually one day in lecture, changing my views forever.</p><div class="no-row-height column-margin column-container"><span class="">Prof. Dunn is often credited as the first to use the Bonferroni Inequality for multiple comparisons</span></div></div>
<p>I am grateful for helpful feedback (agreement with my positions not implied) from Dylan Atmbruster and Boyu Wang</p>
</section>
<section id="appendix" class="level1">
<h1>Appendix</h1>
<p>This Appendix is itended as a start toward developing non-NHST tools for analyst decision making.</p>
<section id="simple-applications-of-the-delta-method" class="level2">
<h2 class="anchored" data-anchor-id="simple-applications-of-the-delta-method">Simple Applications of the Delta Method</h2>
<p>In <a href="#sec-treats" class="quarto-xref">Section&nbsp;9</a>, it was mentioned that one might wish to form a CI for the ratio between some <span class="math inline">\(\beta_i\)</span>, say, and the value <span class="math inline">\(E(Y|X=t)\)</span> of the regression function at some analyst-chosen point <span class="math inline">\(t\)</span>. But in <a href="#sec-curric" class="quarto-xref">Section&nbsp;10.4</a>, concern was raised as to the degree of statistical sophistication needed. Let’s take a look.</p>
<section id="example" class="level3">
<h3 class="anchored" data-anchor-id="example">Example</h3>
<p>The data here is from the 2000 US Census, specifically engineers and programmers in Santa Clara County, California. We will use the first row in our dataset as a “typical” point of comparison. (Again, our emphasis here is on the Delta Method, not on the propriety of this comparison.)</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">'data/svcensus.RData'</span>)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="co"># for simplicity, limit scope</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>svc <span class="ot">&lt;-</span> svcensus[,<span class="fu">c</span>(<span class="st">'age'</span>,<span class="st">'wageinc'</span>,<span class="st">'gender'</span>)]</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># need numeric quantities; replace gender by dummy variable for female</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>svc<span class="sc">$</span>fem <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(svc<span class="sc">$</span>gender<span class="sc">==</span><span class="st">'female'</span>)</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>svc<span class="sc">$</span>gender <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>lmOut <span class="ot">&lt;-</span> <span class="fu">lm</span>(wageinc <span class="sc">~</span> .,<span class="at">data=</span>svc)</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> svc[<span class="dv">1</span>,<span class="sc">-</span><span class="dv">2</span>]  <span class="co"># our reference point</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>x1Pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lmOut,x1) <span class="co"># est. reg. ftn. at x1</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare to compute relevant linear combinations of betahat</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">10</span>,<span class="dv">0</span>)  <span class="co"># effect of 10 years of age</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(x1)</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,b)  <span class="co"># account for intercept term</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>betaHat <span class="ot">&lt;-</span> <span class="fu">coef</span>(lmOut)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a><span class="co"># compute ratio</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>num <span class="ot">&lt;-</span> <span class="fu">t</span>(a) <span class="sc">%*%</span> betaHat</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>den <span class="ot">&lt;-</span> <span class="fu">t</span>(b) <span class="sc">%*%</span> betaHat</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>propor10Age <span class="ot">&lt;-</span> num<span class="sc">/</span>den</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a><span class="co"># compute covariance matrix of ratio</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>cvm <span class="ot">&lt;-</span> <span class="fu">vcov</span>(lmOut)</span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>varn <span class="ot">&lt;-</span> <span class="fu">t</span>(a) <span class="sc">%*%</span> cvm <span class="sc">%*%</span> a  <span class="co"># est. var. of numerator</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>vard <span class="ot">&lt;-</span> <span class="fu">t</span>(b) <span class="sc">%*%</span> cvm <span class="sc">%*%</span> b  <span class="co"># est. var. of denominator</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>cvnd <span class="ot">&lt;-</span> <span class="fu">t</span>(a) <span class="sc">%*%</span> cvm <span class="sc">%*%</span> b  <span class="co"># est. covariance</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a><span class="co"># apply Delta Method</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(msm)</span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>cvProp <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(varn,cvnd,cvnd,vard),<span class="at">nrow=</span><span class="dv">2</span>)</span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">deltamethod</span>(<span class="sc">~</span> x1<span class="sc">/</span>x2,<span class="fu">c</span>(num,den),cvProp)</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(propor10Age <span class="sc">-</span> <span class="fl">1.96</span><span class="sc">*</span>se,propor10Age <span class="sc">+</span> <span class="fl">1.96</span><span class="sc">*</span>se)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.07801891 0.09775186</code></pre>
</div>
</div>
<p>We estimate that the proportional impact of 10 additional years of age is between 7.8 and 9.8%.</p>
<p>The code is somewhat involved, but as noted earlier, well within the capabilities of scientific researchers who use R.</p>
</section>
</section>
<section id="taking-model-inaccuracy-into-account" class="level2">
<h2 class="anchored" data-anchor-id="taking-model-inaccuracy-into-account">Taking model inaccuracy into account</h2>
<p>Continuing <a href="#sec-something" class="quarto-xref">Section&nbsp;10.1.1</a>, we seek estimators with the following characteristics:</p>
<ul>
<li><p>We have a model <span class="math inline">\(\cal{M}\)</span>, again necessarily incorrect, for the distribution of some random quantity <span class="math inline">\(W\)</span>, possibly vector-valued. (We may be interested in just one aspect of the distribution, say the regression function, but for simplicity will refer just to the distribution.)</p></li>
<li><p>Say <span class="math inline">\(\cal{M}\)</span> is parametric with some parameter <span class="math inline">\(\theta\)</span>, again possibly vector-valued.</p></li>
<li><p>Denote the true distribution of <span class="math inline">\(W\)</span> by <span class="math inline">\(F_{pop}\)</span>.</p></li>
<li><p>We estimate <span class="math inline">\(\theta\)</span> by some method, say MLE. We want to find a standard error that <em>does not assume <span class="math inline">\(\cal{M}\)</span> is correct.</em></p></li>
<li><p>Optionally, we may want to also estimate the population “distance” between our model and <span class="math inline">\(F_{pop}\)</span>, including a standard error for that estimate.</p></li>
</ul>
<section id="example-delta-method" class="level3">
<h3 class="anchored" data-anchor-id="example-delta-method">Example: Delta Method</h3>
<p>Say we have a random variable <span class="math inline">\(X\)</span> whose distribution we wish to model as exponential,</p>
<p><span class="math display">\[
f_X(t) = \lambda e^{-t/\lambda}
\]</span></p>
<p>We recognize that this model, as with any, is inherently false, and wish to estimate <span class="math inline">\(\lambda\)</span> accordingly, meaning that we estimate the <span class="math inline">\(\lambda\)</span> of the exponential distribution closest to the the <span class="math inline">\(f_X\)</span>. Say we also want to assess the distance between our model and the true population state of affairs. Denote our data by <span class="math inline">\(X_1,...,X_n\)</span>.</p>
<p>The MLE of <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\widehat{\lambda} = 1/\bar{X}\)</span>, the reciprocal of the sample mean.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Again, what are we actually estimating?">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Again, what are we actually estimating?
</div>
</div>
<div class="callout-body-container callout-body">
<p>Following the discussion in <a href="#sec-something" class="quarto-xref">Section&nbsp;10.1.1</a>, note that by choosing <span class="math inline">\(1/\bar{X}\)</span> as our estimator, the quantity it is estimating is the reciprocal of the population mean <span class="math inline">\(1/\mu = 1/E(X)\)</span>, and our “closest model” to reality is</p>
<p><span class="math display">\[
f_X(t) = \frac{1}{\mu} e^{-t/\mu}
\]</span></p>
<p>Hopefully that is close to the true <span class="math inline">\(f_X(t)\)</span>.</p>
</div>
</div>
<p>As discussed, the classic formulas for the standard error of <span class="math inline">\(\widehat{\lambda}\)</span> are invalid here. But the solution is simple: <span class="math inline">\(\bar{X}\)</span> is asymptotically normal from the Central Limit Theorem, with standard error computable from the data. Thus we can find a CI for E(X), then invert the endpoints to obtain an interval for <span class="math inline">\(\lambda\)</span>.</p>
<p>But what about the distance between the model and actual distributions of <span class="math inline">\(X\)</span>? Under the model, the cumulative distribution function is</p>
<p><span class="math display">\[
F_{model}(t) = 1 - e^{-t/\lambda}
\]</span></p>
<p>estimated by</p>
<p><span class="math display">\[
1 - e^{-t \bar{X}}
\]</span></p>
<p>The estimate of the true cdf is the <em>empirical cdf</em>,</p>
<p><span class="math display">\[
F_{true}(t) = \frac{1}{n} N(t)
\]</span></p>
<p>where <span class="math inline">\(N(t)\)</span> is the number of <span class="math inline">\(X_i\)</span> that are <span class="math inline">\(\leq t\)</span>.</p>
<p>We might compare the two cdfs at various selected points. For simplicity, let’s look at just one point, say <span class="math inline">\(v\)</span>, and take as our “distance” the ratio of model cdf and true cdf. We thus wish to find a standard error for</p>
<p><span class="math display">\[
\frac{1 - e^{-v \bar{X}}}{1 - N(v)/n}
\]</span></p>
<p>This is perfectly set up for the Delta Method. We need the covariance matrix, which we can obtain empirically by applying the R function <strong>cov</strong> to the vectors <span class="math inline">\((X_1,...,X_n\)</span> and <span class="math inline">\((I_1,...,I_n)\)</span>, where <span class="math inline">\(I_j\)</span> is 1 or 0, according to whether <span class="math inline">\(X_j \leq v\)</span>.</p>
</section>
<section id="example-modelreality-distance" class="level3">
<h3 class="anchored" data-anchor-id="example-modelreality-distance">Example: Model/Reality Distance</h3>
<p>Let’s now look at a linear model example, again using the <strong>svcensus</strong> data. Actually, it is originally from my <strong>qeML</strong> package, and it will be convenient to use some functions from that package.</p>
<p>Applying the Delta Method here is less straightforward. Computation of the least-squares estimate <span class="math inline">\(\widehat{\beta}\)</span> of the coefficients vector <span class="math inline">\(\beta\)</span> involves a matrix inverse, making analytical derivatives difficult. Meanwhile, the <strong>deltamethod</strong> function doesn’t allow matrix inverse in its operation. Other implementations of the Delta Method might be used, or a <em>fixed-X regression</em> view might be taken, but the bootstrap seems to be especially attractive in this setting. Let’s give it a try. First, a single-replicate version for clarity:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="fu">suppressMessages</span>(<span class="fu">library</span>(qeML))</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>nreps <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>repOut <span class="ot">&lt;-</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">replicate</span>(nreps,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>      <span class="fu">coef</span>(<span class="fu">qeLin</span>(svcensus,<span class="st">'wageinc'</span>,<span class="at">holdout=</span><span class="dv">100</span>))[<span class="st">'age'</span>])</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span><span class="sc">*</span><span class="fu">mean</span>(repOut)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4698.449</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span><span class="sc">*</span><span class="fu">sd</span>(repOut) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 18.34334</code></pre>
</div>
</div>
<p>So generating a CI for the gender coefficient using the bootstrap is straightforward. But estimating the distance between model and reality here raises an issue regarding what we mean by “distance.”</p>
<p>Certainly we can estimate the distance between the model regression function and the real one. It would not be too different from our previous example of finding the distance between the model cdf and real cdf. One could use, say, k-Nearest Neighbors or kernel regression to estimate the real function.</p>
<p>Say we define distance (in function space) to be <span class="math inline">\(L_1\)</span>, mean absolute difference. We could estimate it this way:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>linOut <span class="ot">&lt;-</span> <span class="fu">qeLin</span>(svc,<span class="st">'wageinc'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="co"># no holdout, to be consistent with linear model</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>knnOut <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(svc,<span class="st">'wageinc'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>) <span class="co"># k-Nearest Nhbr., defaults</span></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(<span class="fu">abs</span>(linOut<span class="sc">$</span>fitted.values <span class="sc">-</span> knnOut<span class="sc">$</span>regests))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 12107.06</code></pre>
</div>
</div>
<p>That difference would seem to be rather large, but before addressing that, let’s consider another distance measure, one more directly related to our analysis above:</p>
<p>In the above code, we found a CI for <span class="math inline">\(\beta_{\textrm{age}}\)</span> – a quantity that has no counterpart in general regression functions. What might be done?</p>
<p>One possibility would be to consider <span class="math inline">\(\beta_{\textrm{age}}\)</span> to represent an <em>average treatment effect</em>, essentially</p>
<p><span class="math display">\[
E[E(\textrm{wage income} | \tilde{X},\textrm{age = 50})] -
E[E(\textrm{wage income} | \tilde{X},\textrm{age = 40}])
\]</span></p>
<p>where <span class="math inline">\(\tilde{X}\)</span> denotes the predictors other than age The outer expectation averages the age difference over the distribution of <span class="math inline">\(X\)</span>. We could use k-NN to get the estimates:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(svcensus)  <span class="co"># 2000 Census dataset</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(svcensus)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "age"     "educ"    "occ"     "wageinc" "wkswrkd" "gender" </code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># keep things simple</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>svc <span class="ot">&lt;-</span> svcensus[,<span class="fu">c</span>(<span class="st">'age'</span>,<span class="st">'occ'</span>,<span class="st">'wageinc'</span>,<span class="st">'gender'</span>)]</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>knnOut <span class="ot">&lt;-</span> <span class="fu">qeKNN</span>(svc,<span class="st">'wageinc'</span>,<span class="at">holdout=</span><span class="cn">NULL</span>) <span class="co"># k-Nearest Nhbr., defaults</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="co"># prepare to compare estimated E(Y|X) for age 40 and 50</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>svc40 <span class="ot">&lt;-</span> svc</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>svc40<span class="sc">$</span>age <span class="ot">&lt;-</span> <span class="dv">40</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>svc50 <span class="ot">&lt;-</span> svc</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>svc50<span class="sc">$</span>age <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>meanY40 <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">predict</span>(knnOut,svc40[,<span class="sc">-</span><span class="dv">3</span>]))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>meanY50 <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">predict</span>(knnOut,svc50[,<span class="sc">-</span><span class="dv">3</span>]))</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>meanY50 <span class="sc">-</span> meanY40</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] -5519.589</code></pre>
</div>
</div>
<p>Quite a difference! Let’s look at the data:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(svc[,<span class="fu">c</span>(<span class="st">'age'</span>,<span class="st">'wageinc'</span>)],<span class="at">cex=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="NPV_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Ah, a nonmonotonic relation! (Plus apparent outliers etc.) We might add a quadratic term in age to our linear model.</p>
</section>
</section>
<section id="making-a-decision-iii" class="level2">
<h2 class="anchored" data-anchor-id="making-a-decision-iii">Making a Decision, III</h2>
<p>As noted earlier, proper analysis is harder, and probably less psychologically satisfying than NHST. In this section, there is also the issue of whether a typical scientific researcher even has the background to be able to perform the analysis. In the case of estimating the distance between two distributions, this may be out of reach for many.</p>
<p>This then is a good example of the need for mathematical statisticians to develop non-NHST tools, say along the above lines.</p>
<p>Of course, there is always the bootstrap as a widely accessible alternative. A question, though, is whether scientific journals will accept such analyses. As long as (possibly anonymized) data, code and random number seed are made available, hopefully this could be worked out.</p>
<section id="ah-this-distance-thing-is-very-useful" class="level3">
<h3 class="anchored" data-anchor-id="ah-this-distance-thing-is-very-useful">Ah, this distance thing is very useful!</h3>
<p>So in the last example, computing a distance between model and reality led to a model check and revision. An NHST might have done the same thing, but as discussed, would be too dependent on <span class="math inline">\(n\)</span>; a test is not telling us whether the two distributions are close or far apart.</p>
<p>By contrast, the estimated <span class="math inline">\(L_1\)</span> distance in the case of the LSAT data (first eight columns, predicting LSAT) is about 0.91, very good considered the mean LSAT score was about 36. By contrast, in the Census data about, the estimated distance was 12017, for a mean wage income of 60312. (It was still large after adding a quadratic term, though smaller than before.)</p>
<p>Putting questions of confounders aside, it would appear that a linear model analysis of the Census data is not very safe, while the LSAT example looks very good.</p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>