[
  {
    "objectID": "NPV.html",
    "href": "NPV.html",
    "title": "No P-Values",
    "section": "",
    "text": "Author bio\nSynopsis:"
  },
  {
    "objectID": "NPV.html#effect-of-measurement-error-especially-bias",
    "href": "NPV.html#effect-of-measurement-error-especially-bias",
    "title": "No P-Values",
    "section": "Effect of measurement error, especially bias",
    "text": "Effect of measurement error, especially bias\n\nBiased sampling designs.\nTranscription errors.\n“I just want know which medical treatment is better.” \\(H_0: \\mu_1\n\\leq \\mu2\\) vs. \\(H_{\\textrm{alt}}: \\mu1 &gt; \\mu2\\). Sampling bias or data errors can easily push an effect across that threshold, one direction of the other.\nBias in physical measurements. No physical measurement is infinitely precise, nor absolutely exactly centered.\nE.g. physics, \\(H_0:\\) gravity waves do not exist.\n\nYes, the existence is either true or false, but our imperfect measurements make \\(H_0\\) false.\nResearch on gravity waves relies on a complex array of detection machinery and on other unproven theories. See the main paper and the methods details.\n(The situation is also complicated by the fact that they use a Bayesian analysis, courting further criticism.)"
  },
  {
    "objectID": "NPV.html#both-an-epistemological-and-practical-problem",
    "href": "NPV.html#both-an-epistemological-and-practical-problem",
    "title": "No P-Values",
    "section": "Both an epistemological and practical problem",
    "text": "Both an epistemological and practical problem\n\nOK, \\(H_0\\) is never true. But why is that a problem?\nPhilosophical: Unscientific – and just plain silly – to test for a trait that we know a priori is false.\nThe American Statistical Association (ASA) rightly takes pride in statistics as a science, saying they promote “the development, application, and dissemination of statistical science,” and that its members “[promote] sound statistical practice to inform public policy and improve human welfare.”\nBut testing a hypothesis that one knows is false certainly isn’t science, nor is it “sound practice.”\nPractical: NHST can be extremely misleading, a grave concern in light of the pivotal role statistics plays in crucial human events, such as in medical cases or legal proceedings."
  },
  {
    "objectID": "NPV.html#running-example-lsat-data",
    "href": "NPV.html#running-example-lsat-data",
    "title": "No P-Values",
    "section": "Running example: LSAT data",
    "text": "Running example: LSAT data\n\nSee the Kaggle entry.\nHere is an overview of the variables:\n\nload('data/lsa.RData')\nnames(lsa)\n\n [1] \"age\"      \"decile1\"  \"decile3\"  \"fam_inc\"  \"lsat\"     \"ugpa\"    \n [7] \"gender\"   \"race1\"    \"cluster\"  \"fulltime\" \"bar\"     \n\n\nThe two decile scores are class standing in the first and third years of law school, and ‘cluster’ refers to the reputed quality of the law school.\n\nThe ‘age’ variable is apparently birth year, with e.g. 67 meaning 1967.\n\nTwo variables of particular interest might be the student’s score on the Law School Admission Test (LSAT) and a logical variable indicating whether the person passed the bar examination."
  },
  {
    "objectID": "NPV.html#wealth-bias-in-the-lsat",
    "href": "NPV.html#wealth-bias-in-the-lsat",
    "title": "No P-Values",
    "section": "Wealth bias in the LSAT?",
    "text": "Wealth bias in the LSAT?\n\nThere is concern that the LSAT and other similar tests may be heavily influenced by family income, thus unfair.\n\n\n\nThere are important causal issues here and beloew, but this aspect is beyond the scope of this document.\n\nLet’s consider the estimated coefficients in a linear model for the LSAT:\n\nw &lt;- lm(lsat ~ .,lsa)  # predict lsat from all other variables\nsummary(w)\n\n\nCall:\nlm(formula = lsat ~ ., data = lsa)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.290  -2.829   0.120   2.888  16.556 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 31.985789   0.448435  71.328  &lt; 2e-16 ***\nage          0.020825   0.005842   3.565 0.000365 ***\ndecile1      0.127548   0.020947   6.089 1.15e-09 ***\ndecile3      0.214950   0.020919  10.275  &lt; 2e-16 ***\nfam_inc      0.300858   0.035953   8.368  &lt; 2e-16 ***\nugpa        -0.278173   0.080431  -3.459 0.000544 ***\ngendermale   0.513774   0.060037   8.558  &lt; 2e-16 ***\nrace1black  -4.748263   0.198088 -23.970  &lt; 2e-16 ***\nrace1hisp   -2.001460   0.203504  -9.835  &lt; 2e-16 ***\nrace1other  -0.868031   0.262529  -3.306 0.000947 ***\nrace1white   1.247088   0.154627   8.065 7.71e-16 ***\ncluster2    -5.106684   0.119798 -42.627  &lt; 2e-16 ***\ncluster3    -2.436137   0.074744 -32.593  &lt; 2e-16 ***\ncluster4     1.210946   0.088478  13.686  &lt; 2e-16 ***\ncluster5     3.794275   0.124477  30.482  &lt; 2e-16 ***\ncluster6    -5.532161   0.210751 -26.250  &lt; 2e-16 ***\nfulltime2   -1.388821   0.116213 -11.951  &lt; 2e-16 ***\nbarTRUE      1.749733   0.102819  17.018  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.197 on 20782 degrees of freedom\nMultiple R-squared:  0.3934,  Adjusted R-squared:  0.3929 \nF-statistic: 792.9 on 17 and 20782 DF,  p-value: &lt; 2.2e-16\n\n\nThere are definitely some salient racial aspects here, but, staying with the income issue, look at the coefficient for family income, 0.3009.\nThe p-value is essentially 0, which in an academic research journal would be heralded with much fanfare, termed “very highly significant,” with a 3-star insignia.\nBut actually, the impact of family income is not significant in practical terms. Here’s why:\nFamily income in this dataset is measured in quintiles. So, this estimated coefficient says that, for example, if we compare people who grew up in the bottom 20% of income with those who were raised in the next 20%, the mean LSAT score rises by only about 1/3 of 1 point – on a test where scores are typically in the 20s, 30s and 40s (top score was 48). The 95% confidence interval (CI), (0.2304,0.3714), again indicates that the effect size here is very small.\nSo family income is not an important factor after all, and the significance test was highly misleading. The result was statistically significant but not practically significant. How did this discrepancy arise?"
  },
  {
    "objectID": "NPV.html#source-of-the-problem",
    "href": "NPV.html#source-of-the-problem",
    "title": "No P-Values",
    "section": "Source of the problem",
    "text": "Source of the problem\n\nThe above analysis tests the hypothesis\n\\[\nH_0: \\beta_{\\textrm{faminc}} = 0\n\\]\nusing the test statistic based on the standard error,\n\\[\nW = \\frac{\\widehat{\\beta}_{\\textrm{faminc}}}\n{\\textrm{s.e.}(\\widehat{\\beta}_{\\textrm{faminc}})}\n\\]\nAs the sample size increases the denominator of \\(W\\) goes to 0. The numerator goes to the population parameter \\(\\beta_{\\textrm{faminc}}\\) – which is nonzero since all \\(H_0\\)s are false.\n\nIn some contexts, the term data-generating process, popular in machine learning circles, may work better than population. I use the latter for simplicity and for its propriety in many contexts.\n\n\\(W\\) then goes to \\(\\pm \\infty\\), and thus the p-value goes to 0. In other words:\n\n\n\n\n\n\n\nThe Term ‘Significant’ Has No Meaning\n\n\n\nGiven a large enough dataset, any \\(H_0\\) will have a small p-value and will be rejected. The term “significant” means nothing.\n\n\n\nThe level of “large enough” depends on other factors, e.g. variability in the dataset, complexity of the model and so on, but the same principle holds."
  },
  {
    "objectID": "NPV.html#example-interaction-effects-in-lsat-data",
    "href": "NPV.html#example-interaction-effects-in-lsat-data",
    "title": "No P-Values",
    "section": "Example: interaction effects in LSAT data",
    "text": "Example: interaction effects in LSAT data\n\nSay we are considering adding an interaction term between race and undergraduate GPA to our above model. Let’s fit this more elaborate model, then compare.\n\nw1 &lt;- lm(lsat ~ .+race1:ugpa,lsa)  # add interactions\nsummary(w1)\n\n\nCall:\nlm(formula = lsat ~ . + race1:ugpa, data = lsa)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.1783  -2.8065   0.1219   2.8879  16.0633 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     26.574993   1.219611  21.790  &lt; 2e-16 ***\nage              0.020612   0.005837   3.531 0.000415 ***\ndecile1          0.127585   0.020926   6.097 1.10e-09 ***\ndecile3          0.213918   0.020902  10.234  &lt; 2e-16 ***\nfam_inc          0.295042   0.035939   8.210 2.35e-16 ***\nugpa             1.417659   0.363389   3.901 9.60e-05 ***\ngendermale       0.513686   0.059986   8.563  &lt; 2e-16 ***\nrace1black       4.121631   1.439354   2.864 0.004194 ** \nrace1hisp        1.378504   1.570833   0.878 0.380191    \nrace1other       2.212299   1.976702   1.119 0.263073    \nrace1white       6.838251   1.201559   5.691 1.28e-08 ***\ncluster2        -5.105703   0.119879 -42.590  &lt; 2e-16 ***\ncluster3        -2.427800   0.074862 -32.430  &lt; 2e-16 ***\ncluster4         1.208794   0.088453  13.666  &lt; 2e-16 ***\ncluster5         3.777611   0.124422  30.361  &lt; 2e-16 ***\ncluster6        -5.565130   0.210945 -26.382  &lt; 2e-16 ***\nfulltime2       -1.406151   0.116132 -12.108  &lt; 2e-16 ***\nbarTRUE          1.743800   0.102855  16.954  &lt; 2e-16 ***\nugpa:race1black -2.876555   0.460281  -6.250 4.20e-10 ***\nugpa:race1hisp  -1.022786   0.494210  -2.070 0.038508 *  \nugpa:race1other -0.941852   0.617940  -1.524 0.127479    \nugpa:race1white -1.737553   0.370283  -4.693 2.72e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.193 on 20778 degrees of freedom\nMultiple R-squared:  0.3948,  Adjusted R-squared:  0.3942 \nF-statistic: 645.4 on 21 and 20778 DF,  p-value: &lt; 2.2e-16\n\n\nThe Black and white interaction terms with undergraduate GPA are “very highly significant.” But does that mean we should include them in our model?\nLet’s check the actual impact of including the interaction terms on the value of \\(\\widehat{E}(Y|X=t)\\), the estimated value of the true regression function \\(E(Y|X=t)\\), at some point \\(t\\) chosen to be “typical.”\nWe will find \\(\\widehat{E}(Y|X=t)\\) under both models and compare:\n\ntypx &lt;- lsa[1,-5]  # set up an example case\npredict(w,typx)  # no-interaction model\n\n      2 \n40.2294 \n\npredict(w1,typx)  # with-interaction model\n\n      2 \n40.2056 \n\n\nAdding the interaction term changed the estimated value of the regression function by only about 0.02 out of a 40.23 baseline.\n(The interaction terms themselves are larger than 0.02, but they absorbed some of the values in the original noninteraction variables.)\nSo, if our goal is prediction, we should not include the interaction terms.\nOn the other hand, in this type of application, our goal is more likely to be assessing treatment effects, the “treatment” here being Black or Hispanic. Then things are more complicated.\nWe see for instance that a 1.0 increase in GPA reduces the effect of being Black by an estimated -2.876555, resulting in a net effect of 4.121631-2.876555 = 1.245076. On an exam having top score of 48, this is not a strong effect, and we may decide that the effect of being Black is minor in this particular context."
  },
  {
    "objectID": "NPV.html#dependence-on-ones-goals",
    "href": "NPV.html#dependence-on-ones-goals",
    "title": "No P-Values",
    "section": "Dependence on one’s goals",
    "text": "Dependence on one’s goals\nAs usual, proper analysis depends on one’s goals.\n\nIf the goal is prediction, a linear model may be quite effective even though there is a substantial discrepancy between it and the true population regression function \\(E(Y|X=t)\\).\nBut if the goal is measurement of treatment effects, we may be producing distorted results if the model discrepancy is large. Addressing this problem involves two components:\n\nClarifying the issue of what is actually being estimated, in light of the inevitable falsity of our model.\nOptionally, formally assessing the degree of discrepancy between our model and reality.\n\n\nWe now elaborate on these last two points.\n\nWe are always estimating something\n\nSay we are estimating a parameter \\(\\theta\\) in some model. Does the inevitable incorrectness of the model mean our estimate is meaningless? No, not at all.\nEven though our model is (always) wrong, we are still estimating something. If we fit a linear regression model \\(E(Y|X=t) = \\beta'X\\) (using ’ for matrix transpose, in this case of a column vector), we are in essence estimating the linear function closest to the true population regression function \\(\\mu(t) = E(Y|X=t)\\). What we are estimating with our \\(\\widehat{\\beta}\\) is then the vector of coefficients for that linear function.\n\nThe term “closest” will be used in a broad sense. It could be defined mathematically in various ways.\n\nStatistical theory asks what the long-run behavior of an estimator is, as the sample size \\(n\\) grows. When we say that our least-squares estimator \\(\\widehat{\\beta}\\) estimates that “closest” \\(\\beta\\), we mean that \\(\\lim_{n \\rightarrow \\infty} \\widehat{\\beta} =\\beta\\).\nThe mathematical details are in the Appendix, but the point is this: If that “closest linear function” is not too far from \\(\\mu(t)\\), we can feel comfortable with analyzing the resulting \\(\\widehat{\\beta}\\) as an approximation.\nThe same principle holds for, say, Maximum Likelihood Estimators (MLE). We are estimating the closest model to reality.\nHowever, our estimation procedure for that closest model should not assume the model is correct, especially in computing standard errors. For instance:\n\nNormally, in the linear model, we estimate the conditional variance \\(Var(Y|X=t) = \\sigma^2\\) using the squared residuals,\n\\[\n\\frac{1}{n-p-1} \\sum_{i=1}^n (Y_i - \\widehat{\\beta}' X_i)^2\n\\]\nwhich is then used to produce standard errors for the \\(\\widehat{\\beta}_i\\).\nBut that only works if the model is correct. If not, then each residual includes the model bias. That in turn invalidates the standard errors of the estimated coefficients.\nIn the case of MLEs, the standard errors come from the information matrix, based on a derivation that assumes the model is correct. So standard errors based on the classic formula are incorrect.\n\nThus alternative ways of computing standard errors must be found. At the very least, one has the bootstrap, but analytical methods would be preferred. This discussed in the Appendix."
  },
  {
    "objectID": "NPV.html#goodness-of-fit-of-the-fundamental-model",
    "href": "NPV.html#goodness-of-fit-of-the-fundamental-model",
    "title": "No P-Values",
    "section": "Goodness-of-fit of the fundamental model",
    "text": "Goodness-of-fit of the fundamental model\n\nOnce again, “all models are wrong,” so in addition to calculating a point estimate and a standard error for \\(\\theta\\), the analyst may optionally wish to assess how close his/her model is to the true population entity.\nClassically, that would be done via NHST, in some sort of goodness-of-fit test, but of course NHST should not be done. One should both calculate a point estimate for the distance and also a standard error for the estimated distance.\n(In many cases, this is done informally through a graphical procedure, e.g. plotting residuals. This is fine, but here we address the issue of formal methods.)"
  },
  {
    "objectID": "NPV.html#how-can-this-idea-be-implemented",
    "href": "NPV.html#how-can-this-idea-be-implemented",
    "title": "No P-Values",
    "section": "How can this idea be implemented?",
    "text": "How can this idea be implemented?\n\nThere is an extensive literature on, and R implementations of, minimum distance estimators, a concept which would seem to dovetail with the above point that with a false model we are estimating the closest instance of our model to the true population entity.\nHowever, many have limitations from our point of view here. Most important, the procedure must not assume correctness of the model. In addition, we would require that the procedure should produce standard errors not only for \\(\\widehat{\\theta}\\), but also for the estimated distance between the asymptotic model fit and the true population process.\nFortunately what we need can be easily developed for large classes of estimators. See the Appendix."
  },
  {
    "objectID": "NPV.html#sec-curric",
    "href": "NPV.html#sec-curric",
    "title": "No P-Values",
    "section": "Curricular improvement",
    "text": "Curricular improvement\nBesides the obvious retooling of statistics curricula in terms of banishing NHST, statistics programs need to modernize in terms of including some material on multivariate random variables. By this I mean in particular the concept of covariance and covariance matrices.\nStudents need to be taught how to find the standard error of a linear combination \\(a' \\widehat{\\theta}\\) of the components of a vector \\(\\widehat{\\theta}\\), knowing its estimated covariance matrix \\(\\widehat{\\Sigma}\\). They should be familiar with quadratic forms \\(a'\\widehat{\\Sigma}a\\), and more generally, \\(a'\\widehat{\\Sigma}b\\).\nI would surmise that even most Master’s programs in Statistics do not cover this. Yet really the concept is not beyond the grasp of students in methodology courses that do not require linear algebra, say a course in regression for non-Statistics majors. If the course uses the R lm function, there is no reason students can’t be taught the associated vcov method.\nA related topic is the Delta Method. It’s very handy for constructing CIs for nonlinear functions of \\(\\widehat{\\theta}\\), but has two main barriers to usage:\n\nCalculation of covariance matrices.\nCalculation of derivatives.\n\nThat first barrier again illustrates the above point regarding the need for the inclusion of rudimentary properties of covariance matrices in Statistics curricula.\nThe second barrier is overcome by use of software that computes numerical derivatives internally, such as R’s msm::deltamethod.\nSome exposure to the bootstrap is also desirable.\nMany analysts who apply statistics are largely self-taught. Again, educational materials in that genre should account for these issues."
  },
  {
    "objectID": "NPV.html#simple-applications-of-the-delta-method",
    "href": "NPV.html#simple-applications-of-the-delta-method",
    "title": "No P-Values",
    "section": "Simple Applications of the Delta Method",
    "text": "Simple Applications of the Delta Method\nIn Section 9, it was mentioned that one might wish to form a CI for the ratio between some \\(\\beta_i\\), say, and the value \\(E(Y|X=t)\\) of the regression function at some analyst-chosen point \\(t\\). But in Section 10.4, concern was raised as to the degree of statistical sophistication needed. Let’s take a look.\n\nExample\nThe data here is from the 2000 US Census, specifically engineers and programmers in Santa Clara County, California. We will use the first row in our dataset as a “typical” point of comparison. (Again, our emphasis here is on the Delta Method, not on the propriety of this comparison.)\n\nload('data/svcensus.RData')\n# for simplicity, limit scope\nsvc &lt;- svcensus[,c('age','wageinc','gender')]\n# need numeric quantities; replace gender by dummy variable for female\nsvc$fem &lt;- as.numeric(svc$gender=='female')\nsvc$gender &lt;- NULL\nlmOut &lt;- lm(wageinc ~ .,data=svc)\nx1 &lt;- svc[1,-2]  # our reference point\nx1Pred &lt;- predict(lmOut,x1) # est. reg. ftn. at x1\n# prepare to compute relevant linear combinations of betahat\na &lt;- c(0,10,0)  # effect of 10 years of age\nb &lt;- as.numeric(x1)\nb &lt;- c(1,b)  # account for intercept term\nbetaHat &lt;- coef(lmOut)\n# compute ratio\nnum &lt;- t(a) %*% betaHat\nden &lt;- t(b) %*% betaHat\npropor10Age &lt;- num/den\n# compute covariance matrix of ratio\ncvm &lt;- vcov(lmOut)\nvarn &lt;- t(a) %*% cvm %*% a  # est. var. of numerator\nvard &lt;- t(b) %*% cvm %*% b  # est. var. of denominator\ncvnd &lt;- t(a) %*% cvm %*% b  # est. covariance\n# apply Delta Method\nlibrary(msm)\ncvProp &lt;- matrix(c(varn,cvnd,cvnd,vard),nrow=2)\nse &lt;- deltamethod(~ x1/x2,c(num,den),cvProp)\nc(propor10Age - 1.96*se,propor10Age + 1.96*se)\n\n[1] 0.07801891 0.09775186\n\n\nWe estimate that the proportional impact of 10 additional years of age is between 7.8 and 9.8%.\nThe code is somewhat involved, but as noted earlier, well within the capabilities of scientific researchers who use R."
  },
  {
    "objectID": "NPV.html#taking-model-inaccuracy-into-account",
    "href": "NPV.html#taking-model-inaccuracy-into-account",
    "title": "No P-Values",
    "section": "Taking model inaccuracy into account",
    "text": "Taking model inaccuracy into account\nContinuing Section 10.1.1, we seek estimators with the following characteristics:\n\nWe have a model \\(\\cal{M}\\), again necessarily incorrect, for the distribution of some random quantity \\(W\\), possibly vector-valued. (We may be interested in just one aspect of the distribution, say the regression function, but for simplicity will refer just to the distribution.)\nSay \\(\\cal{M}\\) is parametric with some parameter \\(\\theta\\), again possibly vector-valued.\nDenote the true distribution of \\(W\\) by \\(F_{pop}\\).\nWe estimate \\(\\theta\\) by some method, say MLE. We want to find a standard error that does not assume \\(\\cal{M}\\) is correct.\nOptionally, we may want to also estimate the population “distance” between our model and \\(F_{pop}\\), including a standard error for that estimate.\n\n\nExample\nSay we have a random variable \\(X\\) whose distribution we wish to model as exponential,\n\\[\nf_X(t) = \\lambda e^{-t/\\lambda}\n\\]\nWe recognize that this model, as with any, is inherently false, and wish to estimate \\(\\lambda\\) accordingly, meaning that we estimate the \\(\\lambda\\) of the exponential distribution closest to the the \\(f_X\\). Say we also want to assess the distance between our model and the true population state of affairs. Denote our data by \\(X_1,...,X_n\\).\nThe MLE of \\(\\lambda\\) is \\(\\widehat{\\lambda} = 1/\\bar{X}\\), the reciprocal of the sample mean.\n\n\n\n\n\n\nAgain, what are we actually estimating?\n\n\n\nFollowing the discussion in Section 10.1.1, note that by choosing \\(1/\\bar{X}\\) as our estimator, the quantity it is estimating is the reciprocal of the population mean \\(1/\\mu = 1/E(X)\\), and our “closest model” to reality is\n\\[\nf_X(t) = \\frac{1}{\\mu} e^{-t/\\mu}\n\\]\nHopefully that is close to the true \\(f_X(t)\\).\n\n\nAs discussed, the classic formulas for the standard error of \\(\\widehat{\\lambda}\\) are invalid here. But the solution is simple here: \\(\\bar{X}\\) is asymptotically normal from the Central Limit Theorem, with standard error computable from the data. Thus we can find a CI for E(X), then invert the endpoints to obtain an interval for \\(\\lambda\\).\nBut what about the distance between the model and actual distributions of \\(X\\)? Under the model, the cumulative distribution function is\n\\[\nF_{model}(t) = 1 - e^{-t/\\lambda}\n\\]\nestimated by\n\\[\n1 - e^{-t \\bar{X}}\n\\]\nThe estimate of the true cdf is the empirical cdf,\n\\[\nF_{true}(t) = \\frac{1}{n} N(t)\n\\]\nwhere \\(N(t)\\) is the number of \\(X_i\\) that are \\(\\leq t\\).\nWe might compare the two cdfs at various selected points. For simplicity, let’s look at just one point, say \\(v\\), and take as our “distance” the ratio of model cdf and true cdf. We thus wish to find a standard error for\n\\[\n\\frac{1 - e^{-v \\bar{X}}}{1 - N(v)/n}\n\\]\nThis is perfectly set up for the Delta Method. We need the covariance matrix, which we can obtain empirically by applying the R function cov to the vectors \\((X_1,...,X_n\\) and \\((I_1,...,I_n)\\), where \\(I_j\\) is 1 or 0, according to whethe \\(X_j \\leq v\\)."
  }
]