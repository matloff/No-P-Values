
---
title: "No P-Values"
subtitle: "Why Statistical Hypothesis Testing Is a Bad Idea"
author: Norm Matloff
toc: true
---

[Author bio](https://heather.cs.ucdavis.edu/matloff.html) 

**Synopsis:**

1. All null hypotheses $H_0$ are false.

2. Null Hypothesis Significance Testing (NHST) is thus an absurdity.

3. Here we probe more deeply than typical criticisms of NHST in the
   literature, starting with this core theme that all $H_0$s are false.
   NHST should be abandoned, not modified.

4. Decisions must be made, but based on relevant analysis, not NHST.

5. The concept of "practical significance vs. statistical significance"
   should be reduced to simply assessing practical significance.

6. Non-NHST tools should be developed in that direction, some presented
   here.

7. Such tools must be used to inform the analyst but not 
   automatically make the decision, usurping the analyst.

# NHST Is (Too) Comfortable

* We all grew up with it.

* Clean, simple, orderly -- an OCDer's dream! :-)

* Highly developed mathematical foundation -- a feeling of "safety in
  math."

# The Central Issue

* We wish to determine whether $H_0$ is true or false.

* But:

::: {.callout-important title='Core Problem with NHST'}

In practice, $H_0$ **is never true**.

:::

* Examples:

   * $H_0:$ new drug effectiveness = old drug effectiveness

     Can't be true true after many decimal places.

   * $H_0:$ $\beta_i = 0$ in linear model

     The linear model itself is false -- useful maybe but false -- but just
     an approximation.

   * $H_0:$ population distribution of human weight $W$ is normal/Gaussian

     Measured $W$ is discrete, not continuous.

     No one has a negative weight, or weighs 10,000 kg.

   * $H_0:$ Efficient Market Hypothesis

     * Example in [pro-NHST essay](https://www.aeaweb.org/articles?id=10.1257/jep.35.3.157)
       by Imbens.
     
     * The EMH is intended only as a rough description, so of course it
       is false in the exact sense.

## Effect of measurement error, especially bias

   * Biased sampling designs.

   * Transcription errors.

   * "I just want know which medical treatment is better." $H_0: \mu_1
     \leq \mu2$ vs. $H_{\textrm{alt}}: \mu1 > \mu2$.  Sampling bias or
     data errors can easily push an effect across that threshold, one
     direction of the other.

   * Bias in physical measurements. No physical measurement is 
     infinitely precise, nor absolutely exactly centered.

   * E.g. physics, $H_0:$ gravity waves do not exist.

      * Yes, the existence is either true or false, but our imperfect
        measurements make $H_0$ false.

      * Research on gravity waves relies on a complex array of
        detection machinery and on other unproven
        theories.  See [the main paper](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.116.061102#fulltext) and 
        [the methods details](https://arxiv.org/pdf/1602.03840).

      * (The situation is also complicated by the fact that they use a
        Bayesian analysis, courting further criticism.)

## Both an epistemological and practical problem

* OK, $H_0$ is never true. But why is that a problem?

* Philosophical: Unscientific -- and just plain silly -- to test for 
  a trait that we know *a priori* is false.

* The American Statistical Association (ASA) rightly takes pride in statistics
  as a *science*, [saying](www.amstat.org/about-asa) they promotes"the
  development, application, and dissemination of statistical science,"
  and that its members "[promote] sound statistical practice to inform 
  public policy and improve human welfare." 

  But testing a hypothesis that one knows is false certainly
  isn't science, nor is it "sound practice." 

* Practical: NHST can be extremely misleading, a grave concern in light
  of the pivotal role statistics plays in crucial human events, such as
  in medical cases or legal proceedings.

# The Easy Part: Statistical vs. Practical Significance

* This is the "easy" part, in the sense that most readers here will
  have learned about this issue in their statistics coursework. In other
  words, it would seem that they don't need convincing.

* Yet they do need to be convinced. While they may distinguish
  between statistical vs. practical significance when analyzing
  treatment effects, many do not understand the importance of this
  distinction in general statistical contexts

* So, let's look in detail at this "easy" part before delving into 
  deeper issues later in this document.

## Running example: LSAT data

* See [the Kaggle entry](https://www.kaggle.com/datasets/danofer/law-school-admissions-bar-passage).

* Here is an overview of the variables:

  ```{r}
  load('data/lsa.RData')
  data(lsa)
  names(lsa)
  ```

* The two
decile scores are class standing in the first and third years of law
school, and 'cluster' refers to the reputed quality of the law school.
[ ]{.column-margin}

* Two variables of particular interest might be the student's score on the
Law School Admission Test (LSAT) and a logical variable indicating
whether the person passed the bar examination.
[The 'age' variable is apparently birth year, with e.g. 67 meaning
1967.]{.column-margin}

## Wealth bias in the LSAT?

* There is concern that the LSAT and other similar tests may be
  heavily influenced by family income, thus unfair. 

::: {.column-margin}

There are important causal issues here, but this aspect is beyond the
scope of this document. 

:::

* Let's consider the estimated coefficients in a linear model for the
  LSAT:

  ```{r}
  w <- lm(lsat ~ .,lsa)  # predict lsat from all other variables
  summary(w)
  ```

* There are definitely some salient racial aspects here, but, staying
  with the income issue, look at the coefficient for family income,
  0.3009.

* The p-value is essentially 0, which in an academic research journal
  would be heralded with much fanfare, termed "very highly
  significant," with a 3-star insignia.  

* But actually, the impact of family income is not significant in
  practical terms.  Here's why:

* Family income in this dataset is measured in quintiles.  So, this
  estimated coefficient says that, for example, if we compare people who
  grew up in the bottom 20% of income with those who were raised in the
  next 20%, the mean LSAT score rises by only about 1/3 of 1 point -- on
  a test where scores are typically in the 20s, 30s and 40s (top score
  was 48). The 95% confidence interval (CI), (0.2304,0.3714), again
  indicates that the effect size here is very small.

* So family income is not an important factor after all, and the
  significance test was highly misleading. The result was statistically
  significant but not practically significant. How did this discrepancy
  arise?

## Source of the problem

* The above analysis tests the hypothesis

  $$
  H_0: \beta_{\textrm{faminc}} = 0
  $$
  
  using the test statistic based on the standard error,
  
  $$
  T = \frac{\widehat{\beta}_{\textrm{faminc}}}
  {\textrm{s.e.}(\widehat{\beta}_{\textrm{faminc}})}
  $$

* As the sample size increases the denominator of $T$ goes to 0.  The
  numerator goes to the population parameter $\beta_{\textrm{faminc}}$
  -- **which is nonzero since all $H_0$s are false**.

* $T$ then goes to $\pm \infty$, and thus the p-value goes to 0. In other
  words:

::: {.callout-important title="The Term 'Significant' Has No Meaning"}

Given a large enough dataset, *any* $H_0$ will have a small p-value
and will be rejected. **The term "significant" means nothing.**

:::

* The level of "large enough" depends on other factors, e.g. variability
  in the dataset, complexity of the model and so on, but the same
  principle holds.

# Making a Decision, I

* One of the biggest myths about us critics of NHST is that we fail to
  recognize that the analyst must make a decision. Of course one must make
  a decision -- but a decision based on relevant information, not on an NHST.
  "Making a decision" as to whether $H_0$ is true, when we know it to be
  false, makes no sense.

* For instance: 

  > In the above analysis, the proper decision is to conclude that family
  > income has negligible effect on LSAT scores, after accounting for
  > other possible factors..

* As noted at the outset of this document, making decisions in this
  manner is much less psychologically satisfying than doing an NHST.
  Rather than the decision being determined automatically in an NHST,
  the analyst must supply his/her own power, devising an *ad hoc*
  criterion for making a decision. 

* But it is the scientifically valid approach, rather
  than relying on "superstition" with NHST.

* It is very telling that in a special 2019 issue of the *American
  Statistician* (Volume 73, Issue sup1) devoted to NHST problems, some
  authors proposed elaborate replacements for NHST. See for instance the
  proposed SGPV method in by Blume *et al*. Clearly the psychological
  urge for an automatic decision-making procedure is clearly very
  strong. But I would argue that it is counterproductive, not really
  addressing the problem at hand.

# The Not-So-Easy Part: Abandon the Term *Significant* Altogether

::: {.callout-important title="Abandon the Term 'Significant'"}

We should focus on "practical significance," and abandon assessing
"[statistical] significance." 

Accordingly we need to idenitfy existing formal tools for assessing
*practical* importance, and develop new such tools.

::: 

* As noted, today's statistically-trained analysts know to watch for
  the phenomenon of statistical significance without practical
  significance -- **in the context of measuring a treatment effect**.
  In the above case, the "treatment effect" was of family income. 

* But many analysts do not realize that this same notion may be
  extended to non-treatment contexts.

* E.g. an analyst who says, "I just want to use NHST to check whether my
  model fits the data" is not only missing the principle of "all $H_0$s
  are false," but is also missing an opportunity to perform much more
  meaningful investigation as to whether the model is useful in the
  given setting.

## Example: interaction effects in LSAT data

* Say we are considering adding an interaction term between race and
  undergraduate GPA to our above model.   Let's fit this more elaborate
  model, then compare.

  ```{r}
  w1 <- lm(lsat ~ .+race1:ugpa,lsa)  # add interactions
  summary(w1)
  ```

* The Black and white interaction terms with undergraduate GPA are "very
  highly significant."  But does that mean we should include them in our
  model?

* Let's check the actual impact of including the
  interaction terms on the value of $\widehat{E}(Y|X=t)$, 
  the estimated value of the true regression function
  $E(Y|X=t)$, at some point $t$ chosen to be "typical."

* We will find $\widehat{E}(Y|X=t)$ under both models and compare:
  
  ```{r}
  typx <- lsa[1,-5]  # set up an example case
  predict(w,typx)  # no-interaction model
  predict(w1,typx)  # with-interaction model
  ```

* Adding the interaction term changed the estimated value of the
  regression function by only about 0.02 out of a 40.23 baseline.  

  (The interaction terms themselves are larger than 0.02, but they
  absorbed some of the values in the original noninteraction variables.)

* So, if our goal is prediction, we should not include the interaction
  terms.

* On the other hand, in this type of application, our goal is more
  likely to be assessing treatment effects, the "treatment" here being
  Black or Hispanic. Then things are more complicated.

  We see for instance that a 1.0 increase in GPA reduces the effect of
  being Black by an estimated -2.876555, resulting in a net effect
  of 4.121631-2.876555 = 1.245076. On an exam having top score of 48,
  this is not a strong effect, and we may decide that the effect of
  being Black is minor.

# Making a Decision, II

* Again, the above decision on whether to include interaction terms is
  not as psychologically satisfying as the NHST approach. We had to
  supply our own power, devising an *ad hoc* way to help us make a decision. 

* But this is the rational, scientific approach, rather than relying on
  a test of a hypothesis that we know *a priori* is false, which is
  absurdly unscientific.

* In the case of scientific research, the full explanation of the
  authors' decision must be made available -- especially data 
  (anonymised if need be) and code.

# Regarding Confidence Intervals

* Never base judgment on whether "the CI contains 0." That is simply
  backdoor NHST.

* Instead, use the CI as intended. Pay close attention to the two
  vital pieces of information contained in the CI:

  a. Its center tells us our point estimate of the population quantity
     of interest.

  b. Its radius gives us an idea of how accurate the point estimate is.

* Item (b) here is actually familiar to the general, nonstatistical
  public, in the from of the *margin of error* in opinion polls. 

* If you were a candidate running for office, wouldn't you want to know
  (b), not just (a)? You should view statistical analysis in the same
  way.

* CIs should play a key role in non-NHST analyses.

* Of course this includes multiple-comparisons CIs.

# Formal Non-NHST Tools 

* Defenders of NHST emphasize that NHST formally addresses uncertainty.
  NHST does this badly, but it is definitely true that any analysis must
  address the random nature of the data. Non-NHST tools must do so.

* So for instance it would be desirable to formalize 
  our assessment of adding interaction terms in the
  LSAT model above.

Below we discuss two broad classes of NHST tools.
(See the Appendix for mathematical details of material in 
this section.)

## Procedures assessing treatment effects

* In the interaction terms example, we informally compared the change in 
  $\widehat{E}(Y|X=t)$ that occurred when we added interaction terms.
  How can we formalize this into a CI?

* A CI for the contribution of an interaction term to an overall 
  estimate of $E(Y|X=t)$ is straightforward, just the point estimate
  plus or minux 1.96 times the standard error, 

  $$
  -2.876555 \pm  1.96*0.460281 = (-3.336836, -2.416274)
  $$

* However, the more relevant quantity might be the net effect of being
  Black and having a 1.0 point increase in GPA, i.e. the 
  computation 4.121631-2.876555 = 1.245076 shown above. Computation is
  still straightforward, if the analyst knows covariance matrices (see
  @sec-curric below.)

* A more sophisticated approach would be to form a CI for the
  *proportional* contribution to $\widehat{E}(Y|X=t)$ of some estimated
  regression coefficient, for some analyst-chosen "typical" value of
  $t$. In the code above, we arbitrarily chose the first row in the
  dataset at our $t$, but the analyst might choose it, say, as the
  vector of means of all the predictor variables.

  We then would like to form a CI for the quantity

  $$
  \frac{\beta_i}{E(Y|X=t)}
  $$

  This can be obtained via the Delta Method, or by the bootstrap.

* The *log-linear model* can be used to explore relationships among
  categorical variables. For instance, this might be used in the LSAT
  data to analyze relationships between race, gender, family income and
  so on.

  Again there is a hierarchy of interaction terms, as in our linear
  model example, so again either the Delta Method or the bootstrap can
  be used. The necessary standard errors for DM can be obtained via
  [the "Poisson trick"](https://github.com/matloff/LogLinFit).

## Goodness-of-fit of the fundamental model

::: {.callout-important title='Worst of the Worse of NHST Usage'}

The problem, "All $H_0$s are false," is especially acute for NHSTs that
test model fit.

:::


* As noted earlier, goodness-of-fit tests can be especially problematic
  in terms of the inappropriate nature of NHST.  What can be done instead?

* First, ask if assessment of the model is needed in the first place.
  For instance, many statistical methods, such as the linear model, are
  robust to normality assumptions, and thus assessing normality is
  superfluous.

* Explicitly recognize that one's model is an approximation, by
  estimating some measure of distance from the population version of
  one's model to the true population probabilities, density or whatever.

## Examples 

  * As the sample size grows, a fitted linear regression
    model will converge to the linear function that best approximates
    the true population regression function.
  
  * A genetic model may be technically false but still serve as a good
    approximation. As the sample size grows, the fitted probabilities will
    converge to the set of model probabilities closest to the true
    population values.

  * In a Markov chain analysis, we might model the holding times as
    exponentially distributed as an approximation. The resulting
    estimated exponential distributions will converge to the closest
    exponential distributions to the true population distributions.

* In all these situations, estimating a distance (broadly defined)
  can help us determine if our (imperfect) model is good enough.

## How can this idea be implemented?

* Setting:

  * Let $\theta$ denote the population quantity of interest and write
    $\widehat{\theta}$ for our sample estimate. 

  * Note that we are informally defining $\theta$ to be the limit of
    $\widehat{\theta}$ as the sample size $n$ does to infinity. 

  * As in the examples above, we do NOT assume the model is correct;
    $\theta$, rather than describing the true data-generating process,
    merely describes a process that hopefully is close enough to it for
    practical use.

* There is an extensive literature on, and R implementations of,
  *minimum distance estimators*. However, most have limitations
  from our point of view here. For instance, the procedure should
  produce standard errors not only for $\theta$, the population quantity
  of interest, but also for the distance from the asymptotic model fit and 
  the true population process.

* Fortunately what we need can be easily developed for large classes of
  estimators. See the Appendix.

## Curricular improvement {#sec-curric}

Besides the obvious retooling of statistics curricula in terms of
banishing NHST, statistics programs need to modernize in terms of
including some material on multivariate random variables. By this I mean
in particular the concept of covariance and covariance matrices.

Students need to be taught how to find the standard error of a linear
combination $a' \widehat{\theta}$ of the components of a vector
$\widehat{\theta}$, knowing its estimated covariance matrix
$\widehat{\Sigma}$. They should be familiar with quadratic 
forms $a'\widehat{\Sigma}a$, and more generally, $a'\widehat{\Sigma}b$.

I would surmise that even most Master's programs in Statistics do not
cover this. Yet really the concept is not beyond the grasp of students
in methodology courses that do not require linear algebra, say a course in
regression for non-Statistics majors. If the course uses the R **lm**
function, there is no reason students can't be taught the associated
**vcov** method.

Many analysts who apply statistics are largely self-taught. Again,
educational materials in that genre should account for these issues.

# Where Is the ASA on This Issue?

The American Statistical Association clearly should play a leading role
in rectifying this grave error in statistical science. And in fact, in
one sense, it has done so.

In 2016, the ASA released [a position
paper](https://www.burtthompson.net/uploads/9/6/8/4/9684389/wasserstein-2016__asa_p-value_statement.pdf) on the issue, 
along with a [cover letter.](https://www.amstat.org/asa/files/pdfs/p-valuestatement.pdf). The latter says in part (emphasis added),

> "'The p-value was never intended to be a substitute for scientific
> reasoning,' said Ron Wasserstein, the ASA’s executive director.
> 'Well-reasoned statistical arguments contain much more than the value of
> a single number and whether that number exceeds an arbitrary threshold.
> *The ASA statement is intended to steer research into a ‘post p<0.05
> era*'...'The contents of the ASA statement and the reasoning behind it
> are not new—statisticians and other scientists have been writing on the
> topic for decades,' [then-ASA President] Utts said. 'But this is the
> first time that the community of statisticians, as represented by the
> ASA Board of Directors, has issued a statement to address these
> issues.'"

Wasserstein (who today is still the ASA executive director), along with
*American Statistician* editor Nicole Lazar, authored the article.

There is considerable overlap there with my points in the present
document.  But the ASA editorial (referred to below as "the 2016
ASA Statement") stopped short of advocating complete abandonment
of the p-value concept. 

Later, the *American Statistician* ran a full issue on the topic, with a
sharper tone in the [cover
editorial](https://www.tandfonline.com/doi/pdf/10.1080/00031305.2019.1583913)
(which I'll call "the 2019 editorial"). The authors, Wasserstein, Lazar
and Allen Schirm,used language such as "eliminating the use of p-values
as a truth arbiter."

Readers of the present document should keep in mind, as I have noted,
that **my criticisms of NHST are much deeper than those of most critics,
which don't go much further than warning analysts of the difference
between statistical and practical significance.** In that light, 
this passsge in the 2019 editorial is especially noteworthy:

> Reviewers of this editorial asked, as some readers of it will, is a
> p-value threshold ever okay to use? We asked some of the authors of
> articles in the special issue that question as well...[some] authors
> suggested that such dichotomized use of p-values was acceptable in
> modelfitting and variable selection strategies, again as automated
> tools, this time for sorting through large numbers of potential models
> or variables. Still others pointed out that p-values with very low
> thresholds are used in fields such as physics, genomics, and imaging as
> a filter for massive numbers of tests.  '[B]inary decision making is
> indispensable in medicine and related fields'...

**The above passage brings up several issues that I refute in the present
document.**

According to Wasserstein (personal communication, April 8, 2025), after
the release of the 2019 editorial,

> ...there were certainly some concerned voices. One of them reached out
> to Karen Kafadar, who was [ASA] president at the time...[She appointed
> a task force to study the matter] The task force composition
> notably lacked diversity of viewpoints on this issue..."

The 2019 editorial led to [a vehement
counter-statement](https://hdsr.mitpress.mit.edu/pub/50vl2b07/release/2)
by the task force ("the 2021 task force report"), authored by some of the most
prominent people in the field.  As Wasserstein points out, the list of
signatores "notably lacked diversity of viewpoints on this issue."

I consider that list to include some of the most talented scholars in
the field. How could such creative -- and thus open-minded -- people be
so closed-minded on this issue? I believe this stems from the strong
emotionally vested feelings on NHST among mathematical statisticians. For
them, NHST is not just something tbey "grew up with," but a notion that
has been absolutely core to their careers. Their work and their
professional stature have stemmed from research that is typically
NHST-centered. They thus had a powerful incentive to counter the 2019
editorial  Though their bias was undoubtely not conscious, it was
nevertheless very profound. 

I hope that these highly talented researchers will instead turn to
developing non- NHST tools.

Finally, it should be noted that neither the 2019 nor 2021 statements 
are official ASA policy; the 2016 ASA Statement is the only such position.
As Wasserstein wrote in his April 2025 message to me,

> "The 2016 ASA Statement on P-Values and Statistical Significance remains
> the only official statement issued by the ASA on this topic. The
> president's task force statement represents the views of its authors,
> not an official ASA position."


# Conclusions

In the classic novel *Don Quixote*, the deranged but well-meaning hero
attacks windmills, which he believes to be evil giants.  American
humorist Mark Twain [viewed the work](en.wikipedia.org/wiki/Don_Quixote)
as having "swept the world's admiration for the mediaeval
chivalry-silliness out of existence." The NHST silliness is no less
deserving of demise.

Statistics textbooks and curricula should finally be purged of the
absurdity of testing hypotheses that we know *a priori* to be false.
Mathematical statisticians should develop new formal non-NHST tools. And
most of all, the ASA should play a leading role in all this, expanding
on its 2016 ASA Statement and becoming more activist on the issue.

# Acknowledgements

I was first introduced to this vital issue long ago in graduate
[Prof. Dunn is often credited as the first to use the Bonferroni
Inequality for multiple comparisons]{.column-margin}
school by the late Professor Olive Jean Dunn of the UCLA Biostatistics
Department. I was doing my dissertation in the Mathematics Department.
but took classes in Biostat. Jean brought up the topic casually one day
in lecture, changing my views forever.

# Appendix

This Appendix is itended as a start toward developing non-NHST tools for
analyst decision making.

## Applications of the Delta Method

In the interactions model discussed above, it was mentioned that one
might wish to form a CI for the ratio between some $\beta_i$, say, and
the value $E(Y|X=t)$ of the regression function at some analyst-chosen
point $i$. How can this be done?W\

